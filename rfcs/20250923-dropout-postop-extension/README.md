# Dropout attribute extension

In order to support dropout fusion in sdpa pattern in PyTorch, the
dropout attribute needs to be extended to align with PyTorch semantics.

The requirements are as follow:
- support offset parameter for Phillox RNG: this is necessary on two accounts:
  - for distributed training/fine-tuning, the tensors are split across
    devices and each device gets a different sub-tensor. In order for
    the generated bits to be independent of how the tensor is split,
    an offset parameter is used, so that philox is called with
    `philox(seed, offset + idx)`, with `idx` the index of the output
    element of the sub-tensor.
  - for seed management. PyTorch does not use a different seed for
    each layer, but uses a global seed. For different bits to be
    generated by each layer with random generation, instead of
    generating a dedicated seed for philox (e.g. `philox_seed =
    hash(seed, operation_id)`), the offset parameter is incremented
    with a second seed that is serially updated by each random op.
- support int64 seed and offset arguments. For seed it is preferable,
  to align with pytorch, for offset arguments it is mandatory in order
  to support the distributed training/fine-tuning scenario for large
  tensors.
- reduce overhead of writting masks: when using non-deterministic
  random values, it is typically required to write the random bits in
  a `mask` output on forward, that then gets reused on backward
  (e.g. with binary mul). However, when random generation is
  deterministic, it can be beneficial to recompute the mask on
  backward, to save memory footprint and bandwidth.
- host-side scalar support: seed and offset are runtime arguments and
  scalars. On devices like GPU, it might be beneficial to accept those
  as host-side scalars, so that they are passed to kernel as kernel
  arguments and avoid overheads of allocating memory on device and
  copy those parameters.


Currently, oneDNN does not have a dropout primitive as there is no
clear indication it would lead to better performance than native
implementation in application side.

However, oneDNN does provide fusion of dropout to Matmul primitive
through the dropout attribute. In this RFC, we won't address a
potential dropout primitive, but only extensions to the existing
dropout attribute.


## Extend dropout attribute to accept offset


To keep backward compatibility, we cannot just require a new execution
argument to be passed. 
Three potential options are possible:
1. Make the `DNNL_ARG_ATTR_DROPOUT_OFFSET` argument optional at
   execution time: when it is passed to execute call, it is read and
   used, and when missing, it is ignored and defaults to offset=0.
   - This does not require any new argument to `attr::set_dropout` or
   `attr::get_dropout`
   - this is resolved at execution time (potentially more opportunity
     for reuse, but extra constraint for implementations)
2. Let the user specify at creation time if offset will be passed at
   execution time or not. 
   - This requires an extra boolean argument `use_offset` to `attr::set_dropout`
   and `attr::get_dropout`.
   - This is more explicit from user perspective as they explicitly
     define execution behavior at creation time.
3. Introduce a new `attr:set_dropout_with_offset`, that is not
   compatible with existing `attr::set_dropout`, and that would take
   `DNNL_ARG_ATTR_DROPOUT_OFFSET` as a mandatory argument at execution
   time. This new method can replace `attr::set_offset` in next major
   release.

Option 1 is not a mecanism we rely on in oneDNN library as it does not
allow user to query primitive descriptor about whether an argument
will be used or not. In general, execution behavior is explicitly
defined at primitive creation time.  Regarding option 3, this would
complexify internals (we would need to introduce proper checks) and
external documentation in case some implementations support one flavor
of the attribute but not the other.

For those reasons, we recommend option 2 (new `use_offsets` boolean
argument to the dropout attribute), and we could make offset mandatory
in future major release if it turns out it is always used.

## Support s64 seed/offset

We propose to extend datatype enum with s64 datatype. 

As offset is newly introduced, we can specify that only s64 is
accepted for offset value, so an `offset_dt` argument is unnecessary
since it would be the only datatype the attribute would accept.

Regarding seed, current dropout attribute API assumes that an `s32`
memory will be passed as `DNNL_ARG_ATTR_DROPOUT_SEED` argument at
execution time, hence why there is no `seed_dt` or `seed_md` argument
to dropout attribute.

For seed we have two potential options here:
1. add `seed_dt` parameter to dropout attribute setter, so that we
  dispatch only implementation that support a given seed
  datatype. That parameter would only accept `s32` (for backward
  compatibility) or `s64`. This argument could be dropped in future
  oneDNN major version if it is decided to accept only `s64` seed.
2. let oneDNN user hash their `s64` seed to `s32` and pass that to
  oneDNN. Given that only a single seed (or a few) are used in the
  application, collision of seeds should be unlikely an issue.

The recommendation here is to go with option 1 and transition to `s64`
seed only in future major release. The impact on implementation should
be low, since current implementations of Philox already rely on `s64`
seed (`s32` seed provided by user is replicated in upper and lower 32
bits of `s64` seed).

## Skip  writing mask to memory

Currently, the dropout attribute takes a memory_descriptor to specify
the mask output layout and datatype. Fortunately, oneDNN also has a
concept of
[`zero_md`](https://github.com/uxlfoundation/oneDNN/blob/6266106643427f85648f05abd6c9f422c502e280/include/oneapi/dnnl/dnnl.hpp#L2793) (or simply `memory::desc()` in C++ API).
to specify that an optional argument will not be passed.

Here we propose to use this mecanism to specify if the mask values
will be written to a dedicated `mask` memory argument, or not. So if the user does not want to write the mask values, they would issue the following call:
```c++
attr.set_dropout(memory::desc());
```

This does not require changes in API and does not break API contract:
oneDNN would just start accepting `zero_md` as argument of
`set_dropout`.

## Accept host-side scalar argument

We can add a `use_host_scalars` argument to specify if scalars
(probability, seed and offset) are passed as host-side scalars or as
device memory objects.

Here we could use one knob per scalar argument but:
- there is no clear use case where some would be on host, and some on
  device memory
- this will complexify implementation with multiple possible
  configurations.

## Overall proposal

The C header will be updated with a
`dnnl_primitive_attr_set_dropout_v2` entry as follow:

```c++
/// Sets probability for output dropout primitive attribute.
///
/// @param attr Primitive attributes.
/// @param mask_desc Output mask memory descriptor
/// @returns #dnnl_success on success and a status describing the error
///     otherwise.
dnnl_status_t DNNL_API dnnl_primitive_attr_set_dropout_v2(
        dnnl_primitive_attr_t attr, const_dnnl_memory_desc_t mask_desc, 
        dnnl_data_type_t seed_dt, int use_offset, int use_host_scalars);
```

The symbol `dnnl_primitive_attr_set_dropout` will just become a call
to `dnnl_primitive_attr_set_dropout_v2` with `seed_dt=f32`,
`use_offset=0` and `use_host_scalars=0`.

On the C++ API side, set_dropout will now call
`dnnl_primitive_attr_set_dropout_v2` and there would be a new overload
with new parameters as follow:

```c++
    /// Sets dropout probability.
    ///
    /// @param mask_desc Output memory descriptor of a dropout mask.
    /// @param seed_dt Datatype used for seed argument. Default is f32
    /// @param is_host_scalars If true, probability, seed and offset are 
    ////       passed as host_scalar memory objects.
    void set_dropout(const memory::desc &mask_desc, data_type seed_dt = f32, 
            bool use_offset = false, bool use_host_scalars = false) {
        error::wrap_c_api(
                dnnl_primitive_attr_set_dropout(get(), mask_desc.get(), 
                    convert_to_c(seed_dt), use_offset, use_host_scalars),
                "could not set dropout primitive attribute");
    }
```


## Example

```c++
using memory;

primitive_attr attr;
attr.set_dropout(memory::desc(), // mask will not be written to memory
        s64, true); // probability/seed/offset will be passed as s64 host scalars. 

matmul::primitive_desc matmul_pd(engine, a_desc, b_desc, c_desc_, attr);
matmul::primitive matmul_p(matmul_pd);

...

memory probability(desc::host_scalar(data_type::f32), .75f);
memory seed(desc::host_scalar(data_type::s64), 1234);
memory offset(desc::host_scalar(data_type::s64), subtensor_offset);

// We won't set DNNL_ARG_ATTR_DROPOUT_MASK
std::unordered_map<int, memory> args = {{DNNL_ARG_SRC, a_mem},
        {DNNL_ARG_WEIGHTS, b_mem}, {DNNL_ARG_DST, c_mem},
        {DNNL_ARG_ATTR_DROPOUT_PROBABILITY, probability},
        {DNNL_ARG_ATTR_DROPOUT_SEED, seed},
        {DNNL_ARG_ATTR_DROPOUT_OFFSET, offset}};

matmul_p.execute(args);
```


## Opens

### Extend dropout attribute support to other primitives

Skipping dropout mask writing on forward is only useful if both
primitives around the dropout node support the dropout attribute.
Currently, the dropout attribute is only supported by Matmul
primitive. However, to support sdpa pattern for example (as described
[here](https://github.com/uxlfoundation/oneDNN/pull/3233), dropout
happens after softmax and before matmul on forward. This implies that
dropout attribute would need to be supported by softmax to support
this pattern.

What other primitive need to support dropout attribute is unclear at
this point.
