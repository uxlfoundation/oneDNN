

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Quantization &#8212; oneDNN v3.11.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/doxyrest-pygments.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/doxyrest_code_copy_button.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script src="_static/target-highlight.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"src": "\\operatorname{src}", "srclayer": "\\operatorname{src\\_layer}", "srclayerattention": "\\operatorname{src\\_layer\\_attention}", "srciter": "\\operatorname{src\\_iter}", "srciterc": "\\operatorname{src\\_iter\\_c}", "weights": "\\operatorname{weights}", "weightslayer": "\\operatorname{weights\\_layer}", "weightsiter": "\\operatorname{weights\\_iter}", "weightspeephole": "\\operatorname{weights\\_peephole}", "weightsprojection": "\\operatorname{weights\\_projection}", "bias": "\\operatorname{bias}", "dst": "\\operatorname{dst}", "dstlayer": "\\operatorname{dst\\_layer}", "dstiter": "\\operatorname{dst\\_iter}", "dstiterc": "\\operatorname{dst\\_iter\\_c}", "diffsrc": "\\operatorname{diff\\_src}", "diffsrclayer": "\\operatorname{diff\\_src\\_layer}", "diffsrclayerattention": "\\operatorname{diff\\_src\\_layer\\_attention}", "diffsrciter": "\\operatorname{diff\\_src\\_iter}", "diffsrciterc": "\\operatorname{diff\\_src\\_iter\\_c}", "diffweights": "\\operatorname{diff\\_weights}", "diffweightslayer": "\\operatorname{diff\\_weights\\_layer}", "diffweightsiter": "\\operatorname{diff\\_weights\\_iter}", "diffweightspeephole": "\\operatorname{diff\\_weights\\_peephole}", "diffweightsprojection": "\\operatorname{diff\\_weights\\_projection}", "diffbias": "\\operatorname{diff\\_bias}", "diffdst": "\\operatorname{diff\\_dst}", "diffdstlayer": "\\operatorname{diff\\_dst\\_layer}", "diffdstiter": "\\operatorname{diff\\_dst\\_iter}", "diffdstiterc": "\\operatorname{diff\\_dst\\_iter\\_c}", "diffgamma": "\\operatorname{diff\\_\\gamma}", "diffbeta": "\\operatorname{diff\\_\\beta}", "workspace": "\\operatorname{workspace}", "srcshape": "\\operatorname{src\\_\\shape}", "dstshape": "\\operatorname{dst\\_\\shape}"}}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dev_guide_attributes_quantization';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '/oneDNN/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'v3.11';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
    <link rel="icon" href="_static/favicons.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Post-ops" href="dev_guide_attributes_post_ops.html" />
    <link rel="prev" title="Dropout" href="dev_guide_attributes_dropout.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/oneAPI-rgb-rev-100.png" class="logo__image only-light" alt=""/>
    <script>document.write(`<img src="_static/oneAPI-rgb-rev-100.png" class="logo__image only-dark" alt=""/>`);</script>
  
  
    <p class="title logo__title">oneDNN v3.11 Documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dev_guide_system_requirements.html">System Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="dev_guide_build.html">Build from Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="dev_guide_build_options.html">Use Build Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="dev_guide_link.html">Link to the Library</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learn oneDNN</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dev_guide_basic_concepts.html">Key Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="page_getting_started_cpp.html">Functional API Basic Workflow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="programming_model.html">oneDNN Concepts</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="page_memory_format_propagation_cpp.html">Memory Format Propagation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="dev_guide_inference_and_training_aspects.html">Inference and Training Aspects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_inference.html">Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_inference_int8.html">Int8 Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_training_bf16.html">Bfloat16 Training</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="dev_guide_attributes.html">Primitive Attributes</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="dev_guide_attributes_fpmath_mode.html">Floating-point Math Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_attributes_accumulation_mode.html">Accumulation Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_attributes_rounding_mode.html">Rounding Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_attributes_deterministic.html">Deterministic Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_attributes_dropout.html">Dropout</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_attributes_post_ops.html">Post-ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_attributes_scratchpad.html">Scratchpad</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_data_types.html">Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="page_cross_engine_reorder_cpp.html">Reorder between CPU and GPU engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_c_and_cpp_apis.html">API</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="interop_with_dpcpp_and_opencl.html">Interoperability with DPC++ and OpenCL</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_opencl_interoperability.html">OpenCL Interoperability</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_dpcpp_interoperability.html">DPC++ Interoperability</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="supported_primitives.html">Supported Primitives</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_convolution.html">Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_inner_product.html">Inner Product</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_rnn.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_batch_normalization.html">Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_binary.html">Binary</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_concat.html">Concat</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_eltwise.html">Eltwise</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_group_normalization.html">Group Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_layer_normalization.html">Layer Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_lrn.html">Local Response Normalization (LRN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_pooling.html">Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_prelu.html">PReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_resampling.html">Resampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_shuffle.html">Shuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_sum.html">Sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_reorder.html">Reorder</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_reduction.html">Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="graph_extension.html">Graph Extension</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="graph_programming_model.html">Programming Model</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_basic_concepts.html">Basic Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_low_precision.html">Low Precision</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="graph_supported_operations.html">Supported Operations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_abs.html">Abs</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_absbackward.html">AbsBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_add.html">Add</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_avgpool.html">AvgPool</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_avgpoolbackward.html">AvgPoolBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_batchnormforwardtraining.html">BatchNormForwardTraining</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_batchnorminference.html">BatchNormInference</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_batchnormtrainingbackward.html">BatchNormTrainingBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_biasadd.html">BiasAdd</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_biasaddbackward.html">BiasAddBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_clamp.html">Clamp</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_clampbackward.html">ClampBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_concat.html">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_convolution.html">Convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_convolutionbackwarddata.html">ConvolutionBackwardData</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_convolutionbackwardweights.html">ConvolutionBackwardWeights</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_convtranspose.html">ConvTranspose</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_convtransposebackwarddata.html">ConvTransposeBackwardData</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_convtransposebackwardweights.html">ConvTransposeBackwardWeights</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_dequantize.html">Dequantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_divide.html">Divide</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_dynamicdequantize.html">DynamicDequantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_dynamicquantize.html">DynamicQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_elu.html">Elu</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_elubackward.html">EluBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_end.html">End</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_exp.html">Exp</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_gelu.html">GELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_gelubackward.html">GELUBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_genindex.html">GenIndex</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_greaterequal.html">GreaterEqual</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_groupnorm.html">GroupNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_hardsigmoid.html">HardSigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_hardsigmoidbackward.html">HardSigmoidBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_hardswish.html">HardSwish</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_hardswishbackward.html">HardSwishBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_interpolate.html">Interpolate</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_interpolatebackward.html">InterpolateBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_layernorm.html">LayerNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_layernormbackward.html">LayerNormBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_leakyrelu.html">LeakyReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_log.html">Log</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_logsoftmax.html">LogSoftmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_logsoftmaxbackward.html">LogSoftmaxBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_matmul.html">MatMul</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_maximum.html">Maximum</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_maxpool.html">MaxPool</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_maxpoolbackward.html">MaxPoolBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_minimum.html">Minimum</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_mish.html">Mish</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_mishbackward.html">MishBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_multiply.html">Multiply</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_pow.html">Pow</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_prelu.html">PReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_prelubackward.html">PReLUBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_quantize.html">Quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reciprocal.html">Reciprocal</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reducel1.html">ReduceL1</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reducel2.html">ReduceL2</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reducemax.html">ReduceMax</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reducemean.html">ReduceMean</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reducemin.html">ReduceMin</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reduceprod.html">ReduceProd</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reducesum.html">ReduceSum</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_relu.html">ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_relubackward.html">ReLUBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_reorder.html">Reorder</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_rmsnorm.html">RMSNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_round.html">Round</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_select.html">Select</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_sigmoid.html">Sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_sigmoidbackward.html">SigmoidBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_softmax.html">SoftMax</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_softmaxbackward.html">SoftMaxBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_softplus.html">SoftPlus</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_softplusbackward.html">SoftPlusBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_sqrt.html">Sqrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_sqrtbackward.html">SqrtBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_square.html">Square</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_squareddifference.html">SquaredDifference</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_staticreshape.html">StaticReshape</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_statictranspose.html">StaticTranspose</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_subtract.html">Subtract</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_tanh.html">Tanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_tanhbackward.html">TanhBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_typecast.html">TypeCast</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_op_wildcard.html">Wildcard</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="graph_fusion_patterns.html">Fusion Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_gated_mlp.html">Gated Multi-Layer Perceptron (Gated-MLP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_gqa.html">Grouped Query Attention (GQA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_sdpa_compressed_kv.html">SDPA with Compressed Key and Value</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_sdpa.html">Scaled Dot-Product Attention (SDPA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_matmul_fusion_patterns.html">MatMul</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_quantized_matmul_fusion_patterns.html">Quantized MatMul</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_convolution_fusion_patterns.html">Convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_quantized_convolution_fusion_patterns.html">Quantized Convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_convtranspose_fusion_patterns.html">ConvTranspose</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_quantized_convtranspose_fusion_patterns.html">Quantized ConvTranspose</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_binary_fusion_patterns.html">Binary</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_unary_fusion_patterns.html">Unary</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_interpolate_fusion_patterns.html">Interpolate</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_reduction_fusion_patterns.html">Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_pool_fusion_patterns.html">Pool</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_norm_fusion_patterns.html">Norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="dev_guide_graph_softmax_fusion_patterns.html">SoftMax</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_graph_dump.html">Graph Dump</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_constant_tensor_cache.html">Constant Tensor Cache</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="dev_guide_examples.html">Examples and Tutorials</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="performance_profiling_and_inspection.html">Performance Profiling and Inspection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_verbose.html">Verbose Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_performance_settings.html">Configuring oneDNN for Benchmarking</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_benchdnn.html">Benchmarking Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_profilers.html">Profiling oneDNN Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_inspecting_jit.html">Inspecting JIT Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="page_performance_profiling_cpp.html">Performance Profiling Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_cpu_dispatcher_control.html">CPU Dispatcher Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_cpu_isa_hints.html">CPU ISA Hints</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_verbose_table.html">Verbose Message Catalogue</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="advanced_topics.html">Advanced Topics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_transition_to_dnnl.html">Transitioning from Intel MKL-DNN to oneDNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_understanding_memory_formats.html">Understanding Memory Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_int8_computations.html">Nuances of int8 Computations</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_primitive_cache.html">Primitive Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_persistent_cache.html">Persistent Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_threadpool.html">Using oneDNN with Threadpool-Based Threading</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_sparsity.html">Sparse memory formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_host_side_scalars.html">Host-Side Scalars Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_experimental.html">Experimental features</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ukernels.html">Ukernels</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_ukernel_basic_concepts.html">Basic Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_ukernel_brgemm.html">Batch-Reduce General Matrix Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev_guide_ukernel_transform.html">Data transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="page_cpu_brgemm_example_cpp.html">BRGeMM ukernel example</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="group_dnnl_api.html">oneDNN API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="group_dnnl_api_primitives.html">Primitives</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_primitives_common.html">Common</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_alg_kind_t.html">enum dnnl_alg_kind_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_normalization_flags_t.html">enum dnnl_normalization_flags_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_primitive_kind_t.html">enum dnnl_primitive_kind_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_prop_kind_t.html">enum dnnl_prop_kind_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_query_t.html">enum dnnl_query_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_normalization_flags.html">enum dnnl::normalization_flags</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_query.html">enum dnnl::query</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_exec_arg_t.html">struct dnnl_exec_arg_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_primitive.html">struct dnnl_primitive</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_primitive_desc.html">struct dnnl_primitive_desc</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_primitive-2.html">struct dnnl::primitive</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_primitive_desc-2.html">struct dnnl::primitive_desc</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_primitive_desc_base.html">struct dnnl::primitive_desc_base</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_attributes.html">Attributes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_algorithm.html">enum dnnl::algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_quantization_mode_t.html">enum dnnl_quantization_mode_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_rounding_mode_t.html">enum dnnl_rounding_mode_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_scratchpad_mode_t.html">enum dnnl_scratchpad_mode_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_prop_kind.html">enum dnnl::prop_kind</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_quantization_mode.html">enum dnnl::quantization_mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_rounding_mode.html">enum dnnl::rounding_mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_scratchpad_mode.html">enum dnnl::scratchpad_mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_post_ops.html">struct dnnl_post_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_primitive_attr.html">struct dnnl_primitive_attr</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_post_ops-2.html">struct dnnl::post_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_primitive_attr-2.html">struct dnnl::primitive_attr</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_reorder.html">Reorder</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_reorder.html">struct dnnl::reorder</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_concat.html">Concat</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_concat.html">struct dnnl::concat</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_sum.html">Sum</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_sum.html">struct dnnl::sum</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_binary.html">Binary</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_binary.html">struct dnnl::binary</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_convolution.html">Convolution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_convolution_backward_data.html">struct dnnl::convolution_backward_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_convolution_backward_weights.html">struct dnnl::convolution_backward_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_convolution_forward.html">struct dnnl::convolution_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_deconvolution.html">Deconvolution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_deconvolution_backward_data.html">struct dnnl::deconvolution_backward_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_deconvolution_backward_weights.html">struct dnnl::deconvolution_backward_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_deconvolution_forward.html">struct dnnl::deconvolution_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_shuffle.html">Shuffle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_shuffle_backward.html">struct dnnl::shuffle_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_shuffle_forward.html">struct dnnl::shuffle_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_eltwise.html">Eltwise</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_eltwise_backward.html">struct dnnl::eltwise_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_eltwise_forward.html">struct dnnl::eltwise_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_softmax.html">Softmax</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_softmax_backward.html">struct dnnl::softmax_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_softmax_forward.html">struct dnnl::softmax_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_pooling.html">Pooling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_pooling_backward.html">struct dnnl::pooling_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_pooling_forward.html">struct dnnl::pooling_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_prelu.html">PReLU</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_prelu_backward.html">struct dnnl::prelu_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_prelu_forward.html">struct dnnl::prelu_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_lrn.html">LRN</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_lrn_backward.html">struct dnnl::lrn_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_lrn_forward.html">struct dnnl::lrn_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_batch_normalization.html">Batch Normalization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_batch_normalization_backward.html">struct dnnl::batch_normalization_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_batch_normalization_forward.html">struct dnnl::batch_normalization_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_group_normalization.html">Group Normalization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_group_normalization_backward.html">struct dnnl::group_normalization_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_group_normalization_forward.html">struct dnnl::group_normalization_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_layer_normalization.html">Layer Normalization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_layer_normalization_backward.html">struct dnnl::layer_normalization_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_layer_normalization_forward.html">struct dnnl::layer_normalization_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_inner_product.html">Inner Product</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_inner_product_backward_data.html">struct dnnl::inner_product_backward_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_inner_product_backward_weights.html">struct dnnl::inner_product_backward_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_inner_product_forward.html">struct dnnl::inner_product_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_rnn.html">RNN</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_rnn_direction_t.html">enum dnnl_rnn_direction_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_rnn_flags_t.html">enum dnnl_rnn_flags_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_rnn_direction.html">enum dnnl::rnn_direction</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_rnn_flags.html">enum dnnl::rnn_flags</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_augru_backward.html">struct dnnl::augru_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_augru_forward.html">struct dnnl::augru_forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_gru_backward.html">struct dnnl::gru_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_gru_forward.html">struct dnnl::gru_forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_lbr_augru_backward.html">struct dnnl::lbr_augru_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_lbr_augru_forward.html">struct dnnl::lbr_augru_forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_lbr_gru_backward.html">struct dnnl::lbr_gru_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_lbr_gru_forward.html">struct dnnl::lbr_gru_forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_lstm_backward.html">struct dnnl::lstm_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_lstm_forward.html">struct dnnl::lstm_forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_rnn_primitive_desc_base.html">struct dnnl::rnn_primitive_desc_base</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_vanilla_rnn_backward.html">struct dnnl::vanilla_rnn_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_vanilla_rnn_forward.html">struct dnnl::vanilla_rnn_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_matmul.html">Matrix Multiplication</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_matmul.html">struct dnnl::matmul</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_resampling.html">Resampling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_resampling_backward.html">struct dnnl::resampling_backward</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_resampling_forward.html">struct dnnl::resampling_forward</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_reduction.html">Reduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_reduction.html">struct dnnl::reduction</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="group_dnnl_api_primitive_cache.html">Primitive Cache</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="group_dnnl_api_profiling.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="enum_dnnl_profiling_data_kind.html">enum dnnl::profiling_data_kind</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="group_dnnl_api_blas.html">BLAS functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="group_dnnl_api_common.html">Common API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_engine.html">Engine</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_engine_kind_t.html">enum dnnl_engine_kind_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_engine.html">struct dnnl_engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_engine-2.html">struct dnnl::engine</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_stream.html">Stream</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_stream_flags_t.html">enum dnnl_stream_flags_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_stream.html">struct dnnl_stream</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_stream-2.html">struct dnnl::stream</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_fpmath_mode.html">Floating-point Math Mode</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_fpmath_mode_t.html">enum dnnl_fpmath_mode_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_fpmath_mode.html">enum dnnl::fpmath_mode</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_service.html">Service</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_cpu_isa.html">enum dnnl::cpu_isa</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_cpu_isa_hints.html">enum dnnl::cpu_isa_hints</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_cpu_isa_hints_t.html">enum dnnl_cpu_isa_hints_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_cpu_isa_t.html">enum dnnl_cpu_isa_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_status.html">enum dnnl::status</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_version_t.html">struct dnnl_version_t</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_utils.html">Utilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_status_t.html">enum dnnl_status_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_error.html">struct dnnl::error</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_handle.html">template struct dnnl::handle</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_handle_traits.html">template struct dnnl::handle_traits</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_accumulation_mode.html">Accumulation Mode</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_accumulation_mode.html">enum dnnl::accumulation_mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_accumulation_mode_t.html">enum dnnl_accumulation_mode_t</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_data_types.html">Data types</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_data_type_t.html">enum dnnl_data_type_t</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_memory.html">Memory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_format_kind_t.html">enum dnnl_format_kind_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_format_tag_t.html">enum dnnl_format_tag_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_profiling_data_kind_t.html">enum dnnl_profiling_data_kind_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_sparse_encoding_t.html">enum dnnl_sparse_encoding_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_memory.html">struct dnnl_memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_memory_desc.html">struct dnnl_memory_desc</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_memory-2.html">struct dnnl::memory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="group_dnnl_graph_api.html">Graph API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_allocator.html">Allocator</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="class_dnnl_graph_allocator.html">class dnnl::graph::allocator</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_engine.html">Engine</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_logical_tensor.html">Logical Tensor</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_graph_layout_type_t.html">enum dnnl_graph_layout_type_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_graph_tensor_property_t.html">enum dnnl_graph_tensor_property_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_graph_logical_tensor_t.html">struct dnnl_graph_logical_tensor_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="class_dnnl_graph_logical_tensor.html">class dnnl::graph::logical_tensor</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_tensor.html">Tensor</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="class_dnnl_graph_tensor.html">class dnnl::graph::tensor</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_op.html">Op</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_graph_op_attr_t.html">enum dnnl_graph_op_attr_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_graph_op_kind_t.html">enum dnnl_graph_op_kind_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="class_dnnl_graph_op.html">class dnnl::graph::op</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_partition.html">Partition</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_graph_partition_policy_t.html">enum dnnl_graph_partition_policy_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="class_dnnl_graph_partition.html">class dnnl::graph::partition</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_compiled_partition.html">Compiled Partition</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_graph_inplace_pair_t.html">struct dnnl_graph_inplace_pair_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="class_dnnl_graph_compiled_partition.html">class dnnl::graph::compiled_partition</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_graph.html">Graph</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="class_dnnl_graph_graph.html">class dnnl::graph::graph</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_compiled_partition_cache.html">Compiled Partition Cache</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_constant_tensor_cache.html">Constant Tensor Cache</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_dump_mode.html">Dump Mode</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_graph_dump_mode_t.html">enum dnnl_graph_dump_mode_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_graph_graph_dump_mode.html">enum dnnl::graph::graph_dump_mode</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_utils.html">Utilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_status.html">Status</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_graph_status.html">enum dnnl::graph::status</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_graph_api_interop.html">Runtime interoperability API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="group_dnnl_graph_api_ocl_interop.html">OpenCL interoperability API</a></li>
<li class="toctree-l4"><a class="reference internal" href="group_dnnl_graph_api_sycl_interop.html">SYCL interoperability API</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="namespace_dnnl_graph.html">namespace dnnl::graph</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="group_dnnl_api_interop.html">Runtime interoperability API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_ocl_interop.html">OpenCL interoperability API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="namespace_dnnl_ocl_interop.html">namespace dnnl::ocl_interop</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_ocl_interop_memory_kind_t.html">enum dnnl_ocl_interop_memory_kind_t</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_sycl_interop.html">SYCL interoperability API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="namespace_dnnl_sycl_interop.html">namespace dnnl::sycl_interop</a></li>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_sycl_interop_memory_kind_t.html">enum dnnl_sycl_interop_memory_kind_t</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_threadpool_interop.html">Threadpool interoperability API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="namespace_dnnl_threadpool_interop.html">namespace dnnl::threadpool_interop</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="group_dnnl_api_ukernel.html">Ukernels</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_ukernel_brgemm.html">BRGeMM ukernel</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_ukernel_brgemm.html">struct dnnl::ukernel::brgemm</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_brgemm.html">struct dnnl_brgemm</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_ukernel_transform.html">Transform ukernel</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_transform.html">struct dnnl_transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_ukernel_transform.html">struct dnnl::ukernel::transform</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="group_dnnl_api_ukernel_utils.html">ukernel utils</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="enum_dnnl_ukernel_pack_type.html">enum dnnl::ukernel::pack_type</a></li>
<li class="toctree-l4"><a class="reference internal" href="struct_dnnl_ukernel_attr_params-2.html">struct dnnl::ukernel::attr_params</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="namespace_dnnl_ukernel.html">namespace dnnl::ukernel</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="enum_dnnl_pack_type_t.html">enum dnnl_pack_type_t</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="struct_dnnl_ukernel_attr_params.html">struct dnnl_ukernel_attr_params</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="namespace_dnnl.html">namespace dnnl</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="namespace_oneapi.html">namespace oneapi</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
        <div class="header-article-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/uxlfoundation/oneDNN" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/uxlfoundation/oneDNN/issues/new?title=Issue%20on%20page%20%2Fdev_guide_attributes_quantization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/dev_guide_attributes_quantization.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Quantization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-model">Quantization Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#static-quantization">Static Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-quantization">Dynamic Quantization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-numerical-behavior-notes">General Numerical Behavior Notes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relevant-apis-and-supported-granularity-levels">Relevant APIs and Supported Granularity Levels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#argument-scaling">Argument Scaling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#available-scaling-api-methods">Available Scaling API Methods</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-scaling-granularity-levels">Supported Scaling Granularity Levels</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#per-tensor-scaling">Per-tensor Scaling</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#per-channel-scaling">Per-Channel Scaling</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#block-scaling">Block Scaling</a><ul class="nav section-nav flex-column">
<li class="toc-h6 nav-item toc-entry"><a class="reference internal nav-link" href="#special-case-mx-compatible-block-scaling-or-dynamic-quantization">Special Case: MX-compatible Block Scaling (or Dynamic Quantization)</a></li>
</ul>
</li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-dimensional-scaling">Multi-Dimensional Scaling</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#argument-zero-points">Argument Zero-Points</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#available-zero-point-api-methods">Available Zero-Point API Methods</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-zero-point-granularity-levels">Supported Zero-Point Granularity Levels</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-case-host-side-scalar-scaling-factor-and-zero-point">Special Case: Host-side Scalar Scaling Factor and Zero-point</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precomputed-reductions">Precomputed Reductions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#available-precomputed-reductions-api-method">Available Precomputed Reductions API Method</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-workflows-examples">Quantization Workflows Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#breakdown-of-convolution-with-int8-quantization">Breakdown of Convolution with INT8 Quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#per-channel-scaling-specifics">Per-Channel Scaling Specifics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weights-preparation-with-per-output-channel-scaling">Weights Preparation with Per-output-channel Scaling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution-with-per-output-channel-quantization">Convolution with Per-output-channel Quantization</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication-with-weight-only-quantization-woq">Matrix Multiplication with Weight-only Quantization (WoQ)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication-with-precomputed-reductions-and-advanced-quantization">Matrix Multiplication with Precomputed Reductions and Advanced Quantization</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="quantization">
<span id="doxid-dev-guide-attributes-quantization"></span><span id="index-0"></span><h1>Quantization<a class="headerlink" href="#quantization" title="Permalink to this heading">#</a></h1>
<p><span class="target" id="doxid-dev-guide-attributes-quantization-1dgaq-intro"></span></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Some primitives support input and output tensors with <code class="docutils literal notranslate"><span class="pre">int8</span></code> data types, both signed and unsigned, enabling reduced-precision inference on supported hardware.</p>
<p>Similarly, some primitives support <a class="reference external" href="https://www.opencompute.org/documents/ocp-8-bit-floating-point-specification-ofp8-revision-1-0-2023-06-20-pdf">Open Compute Project (OCP) 8-bit Floating Point (f8) data types</a> designed to accelerate AI workloads, including training and inference of large neural networks. Lowering precision to 8 bits with <code class="docutils literal notranslate"><span class="pre">f8</span></code> enables faster computation and reduced memory usage.</p>
<p>See also:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.intel.com/content/dam/develop/external/us/en/documents/lower-numerical-precision-deep-learning-jan2018-754765.pdf">Lower Numerical Precision Deep Learning Inference and Training</a></p></li>
</ul>
</section>
<section id="quantization-model">
<h2>Quantization Model<a class="headerlink" href="#quantization-model" title="Permalink to this heading">#</a></h2>
<p>oneDNN supports two main categories of quantization:</p>
<ul class="simple">
<li><p>Static Quantization (see <a class="reference internal" href="enum_dnnl_quantization_mode_t.html#doxid-group-dnnl-api-attributes-1gga5342e1d6b2a09ea01660b3a3c2400826a69f0c0079f39bec2332481404e199315"><span class="std std-ref">quantization_mode::dnnl_quantization_mode_static_sazp</span></a>) with scales only (symmetric) or scales and zero-points (asymmetric), where scales are applied after zero-point.</p></li>
<li><p>Dynamic Quantization (see <a class="reference internal" href="enum_dnnl_quantization_mode_t.html#doxid-group-dnnl-api-attributes-1gga5342e1d6b2a09ea01660b3a3c2400826a9d29b3c3bf3c43cab388533e093cd8a6"><span class="std std-ref">quantization_mode::dnnl_quantization_mode_dynamic_mx</span></a>) compliant with the <a class="reference external" href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">OCP Microscaling (MX) Formats Specification</a>.</p></li>
</ul>
<p>To support quantization, primitives should be created and executed as follows:</p>
<ul class="simple">
<li><p>During primitive descriptor creation source, weights or destination memory descriptors use low precision datatype (e.g., <code class="docutils literal notranslate"><span class="pre">s8</span></code> or <code class="docutils literal notranslate"><span class="pre">f8_e4m3</span></code>).</p></li>
<li><p>During primitive descriptor creation group size, data types, and broadcasting masks of the scaling factors and zero-point are provided using primitive attributes.</p></li>
<li><p>During primitive execution the actual quantization parameters are provided as arguments to the execute function.</p></li>
</ul>
<p>For performance reasons, each primitive implementation typically supports only a subset of quantization parameter masks, group sizes and data type combinations. Which combination is supported and optimized is listed in each primitive documentation page.</p>
<p>This guide does not cover how the appropriate scaling factor can be found. Refer to the materials in the <a class="reference internal" href="#doxid-dev-guide-attributes-quantization-1dgaq-intro"><span class="std std-ref">Introduction</span></a>.</p>
<section id="static-quantization">
<h3>Static Quantization<a class="headerlink" href="#static-quantization" title="Permalink to this heading">#</a></h3>
<p>The only formula for static quantization currently supported by oneDNN is with scales applied after zero-point as follows:</p>
<div class="math notranslate nohighlight">
\[x_{f32}[:] = scale_{x} \cdot (x_{quant}[:] - zp_{x})\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{f32}\)</span> and <span class="math notranslate nohighlight">\(x_{quant}\)</span> are the non-quantized and quantized representation of <span class="math notranslate nohighlight">\(x\)</span> respectively, <span class="math notranslate nohighlight">\(scale_{x}\)</span> is a scaling factor in a floating-point format, <span class="math notranslate nohighlight">\(zp_{x}\)</span> is a zero point (typically in integral format), and <span class="math notranslate nohighlight">\([:]\)</span> is used to denote element-wise application of the formula to the arrays.</p>
<p>In this model, oneDNN assumes that quantization parameters are inputs provided by the user and the library does not compute those scaling factors and zero-points as part of primitive computation.</p>
<p>These quantization parameters can either be computed ahead of time using calibration tools or at runtime based on the actual minimum and maximum values of a tensor. Either method can be used in conjunction with oneDNN static quantization, as long as the quantization parameters are passed as input to the oneDNN primitives at execution time.</p>
</section>
<section id="dynamic-quantization">
<h3>Dynamic Quantization<a class="headerlink" href="#dynamic-quantization" title="Permalink to this heading">#</a></h3>
<p>oneDNN supports two dynamic quantization modes, for scales only (no zero-point) following the formula:</p>
<div class="math notranslate nohighlight">
\[x_{f32}[:] = scale_{x} \cdot x_{quant}[:]\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{f32}\)</span> and <span class="math notranslate nohighlight">\(x_{quant}\)</span> are the non-quantized and quantized representation of <span class="math notranslate nohighlight">\(x\)</span> respectively, and <span class="math notranslate nohighlight">\(scale_{x}\)</span> is a scaling factor.</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">quantization_mode::dynamic_mx</span></code>, <span class="math notranslate nohighlight">\(scale_{x}\)</span> is computed following the <a class="reference external" href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">OCP MX Formats Specification</a>, namely <span class="math notranslate nohighlight">\(scale_{x}\)</span> :</p>
<ul class="simple">
<li><p>has <code class="docutils literal notranslate"><span class="pre">e8m0</span></code> datatype,</p></li>
<li><p>is computed for each group of size <code class="docutils literal notranslate"><span class="pre">32</span></code>,</p></li>
<li><p>is computed as the largest power-of-two less than or equal to the maximum absolute value of the group divided by the largest power-of-two representable in the <span class="math notranslate nohighlight">\(x_{quant}\)</span> datatype, e.g., <span class="math notranslate nohighlight">\(E8M0(amax(x_{quant}[:])) / E8M0(MAX\_QUANT\_DT)\)</span>.</p></li>
</ul>
<p>When using <code class="docutils literal notranslate"><span class="pre">quantization_mode::dynamic_fp</span></code>, <span class="math notranslate nohighlight">\(scale_{x}\)</span> is computed in <code class="docutils literal notranslate"><span class="pre">f32</span></code> first and then converted to a scale datatype, namely <span class="math notranslate nohighlight">\(scale_{x}\)</span> :</p>
<ul class="simple">
<li><p>has <code class="docutils literal notranslate"><span class="pre">f8_e4m3</span></code> datatype,</p></li>
<li><p>is computed for each group of size <code class="docutils literal notranslate"><span class="pre">16</span></code>,</p></li>
<li><p>is computed as <span class="math notranslate nohighlight">\(SCALE\_DT(amax(x_{quant}[:]) / MAX\_QUANT\_DT)\)</span>.</p></li>
</ul>
</section>
</section>
<section id="general-numerical-behavior-notes">
<h2>General Numerical Behavior Notes<a class="headerlink" href="#general-numerical-behavior-notes" title="Permalink to this heading">#</a></h2>
<p>Primitive implementations are allowed to convert inputs to wider data types (e.g., <code class="docutils literal notranslate"><span class="pre">int8</span></code> to <code class="docutils literal notranslate"><span class="pre">int16</span></code> or <code class="docutils literal notranslate"><span class="pre">int32</span></code>), when those conversions do not impact accuracy.</p>
<p>During execution, primitives implementations avoid integer overflows and maintain integer accuracy by using wider data types (e.g., <code class="docutils literal notranslate"><span class="pre">int32</span></code>) for intermediate values and accumulators.</p>
<p>Results are then converted as necessary before the result is written to the output memory objects.</p>
<p>The scales are applied in single precision floating point data type (<a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a>) before downconversion to the destination data type. When converting to integral data types, implementations typically saturate, whereas for floating-point data types, underflow/overflow can occur. To force saturation in floating-point data types use <a class="reference internal" href="dev_guide_attributes_post_ops.html#doxid-dev-guide-attributes-post-ops-1dev-guide-attributes-post-ops-eltwise"><span class="std std-ref">dev_guide_attributes_post_ops_eltwise</span></a> with clip algorithm. Rounding happens according to <a class="reference internal" href="dev_guide_attributes_rounding_mode.html#doxid-dev-guide-attributes-rounding-mode"><span class="std std-ref">rounding mode attribute</span></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Depending on the architecture, the behavior of <code class="docutils literal notranslate"><span class="pre">int8</span></code> computations might slightly vary. For more details, refer to <a class="reference internal" href="dev_guide_int8_computations.html#doxid-dev-guide-int8-computations"><span class="std std-ref">Nuances of int8 Computations</span></a>.</p>
</div>
<p>When multiple operations are fused in a single primitive using the <a class="reference internal" href="dev_guide_attributes_post_ops.html#doxid-dev-guide-attributes-post-ops"><span class="std std-ref">post ops attribute</span></a>, those are assumed to be computed in <code class="docutils literal notranslate"><span class="pre">f32</span></code> precision. As a result the destination quantization parameters are applied after the post-ops as follows:</p>
<div class="math notranslate nohighlight">
\[\dst[:] = post\_ops(OP(src[:], weights[:], ...)) / scale_{\dst} + zp_{\dst}\]</div>
<p>Quantizing and dequantizing values between post-operations can be achieved using one of <a class="reference internal" href="dev_guide_attributes_post_ops.html#doxid-dev-guide-attributes-post-ops-1dev-guide-attributes-post-ops-eltwise"><span class="std std-ref">eltwise</span></a>, <a class="reference internal" href="dev_guide_attributes_post_ops.html#doxid-dev-guide-attributes-post-ops-1dev-guide-attributes-post-ops-binary"><span class="std std-ref">binary</span></a>, or the scale parameter of the appropriate post-operation.</p>
</section>
<section id="relevant-apis-and-supported-granularity-levels">
<h2>Relevant APIs and Supported Granularity Levels<a class="headerlink" href="#relevant-apis-and-supported-granularity-levels" title="Permalink to this heading">#</a></h2>
<p>oneDNN provides APIs to set scales, zero-points, and precomputed reductions for different quantization levels from global (per-tensor) to fine-grained block-wise.</p>
<p><span class="target" id="doxid-dev-guide-attributes-quantization-1dgaq-scaling"></span></p>
<section id="argument-scaling">
<h3>Argument Scaling<a class="headerlink" href="#argument-scaling" title="Permalink to this heading">#</a></h3>
<p>The library uses <a class="reference internal" href="dev_guide_attributes.html#doxid-dev-guide-attributes"><span class="std std-ref">Primitive Attributes</span></a> API for setting the scaling factors for most of the primitives. The supporting attributes can be found in the documentation for each primitive. The unsupported cases are handled according to the <a class="reference internal" href="dev_guide_attributes.html#doxid-dev-guide-attributes-1dev-guide-attributes-error-handling"><span class="std std-ref">attributes error handling section</span></a>.</p>
<section id="available-scaling-api-methods">
<h4>Available Scaling API Methods<a class="headerlink" href="#available-scaling-api-methods" title="Permalink to this heading">#</a></h4>
<p>oneDNN provides the following methods for setting scaling factors:</p>
<pre class="highlight literal-block"><span></span><span class="c1">// Legacy method with simple mask-based scaling</span>
<span class="kt">void</span> <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ac3dc9efa6702a5eba6f289f1b3907590"><span class="std std-ref">dnnl::primitive_attr::set_scales_mask</span></a><span></span><span class="p">(</span><span class="kt">int</span> <span class="n">arg</span><span class="p">,</span> <span class="kt">int</span> <span class="n">mask</span><span class="p">);</span>

<span class="c1">// Generic method with groups support</span>
<span class="kt">void</span> <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a35152c75fb7d3e44aa7e51e9ac25c3b0"><span class="std std-ref">dnnl::primitive_attr::set_scales</span></a><span></span><span class="p">(</span><span class="kt">int</span> <span class="n">arg</span><span class="p">,</span> <span class="kt">int</span> <span class="n">mask</span><span class="p">,</span>
                                      <span class="k">const</span> <a class="reference internal" href="struct_dnnl_memory-2.html#doxid-structdnnl-1-1memory-1a7d9f4b6ad8caf3969f436cd9ff27e9bb"><span class="std std-ref">dnnl::memory::dims</span></a><span></span> <span class="o">&amp;</span><span class="n">groups</span><span class="p">,</span>
                                      <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dce"><span class="std std-ref">dnnl::memory::data_type</span></a><span></span> <span class="n">data_type</span> <span class="o">=</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">,</span>
                                      <span class="kt">bool</span> <span class="n">is_on_host</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
                                      <a class="reference internal" href="enum_dnnl_quantization_mode.html#doxid-group-dnnl-api-attributes-1ga43df4b809a4544d34bbc106d3e409b2c"><span class="std std-ref">quantization_mode</span></a><span></span> <span class="n">qmode</span> <span class="o">=</span> <span class="n">quantization_mode</span><span class="o">::</span><span class="n">static_sazp</span><span class="p">);</span>

<span class="c1">// Convenience method for single host-side scalar</span>
<span class="kt">void</span> <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a7b035390cde177453afae9c5b5a7c29e"><span class="std std-ref">dnnl::primitive_attr::set_host_scale</span></a><span></span><span class="p">(</span><span class="kt">int</span> <span class="n">arg</span><span class="p">,</span>
                                          <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dce"><span class="std std-ref">dnnl::memory::data_type</span></a><span></span> <span class="n">data_type</span> <span class="o">=</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">);</span></pre>
<p>Key parameters of the scaling API methods are summarized below:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Options*</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">arg</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DNNL_ARG_SRC</span></code> , <code class="docutils literal notranslate"><span class="pre">DNNL_ARG_WEIGHTS</span></code> , <code class="docutils literal notranslate"><span class="pre">DNNL_ARG_DST</span></code> , <code class="docutils literal notranslate"><span class="pre">DNNL_ARG_BIAS</span></code></p></td>
<td><p>Tensor to scale</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mask</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code> , <code class="docutils literal notranslate"><span class="pre">1&lt;&lt;dim</span></code> , <code class="docutils literal notranslate"><span class="pre">(1&lt;&lt;d1)+(1&lt;&lt;d2)</span></code></p></td>
<td><p>Scaling granularity: global, per-dimension, multi-dimensional</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">groups</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code> , <code class="docutils literal notranslate"><span class="pre">{G}</span></code> , <code class="docutils literal notranslate"><span class="pre">{G1,G2,...}</span></code></p></td>
<td><p>Block quantization: none, single-size, multi-dimensional blocks</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">data_type</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">f32</span></code> , <code class="docutils literal notranslate"><span class="pre">bf16</span></code> , <code class="docutils literal notranslate"><span class="pre">f16</span></code> , <code class="docutils literal notranslate"><span class="pre">f8_e5m2</span></code> , <code class="docutils literal notranslate"><span class="pre">f8_e4m3</span></code> , <code class="docutils literal notranslate"><span class="pre">e8m0</span></code></p></td>
<td><p>Scaling factor data type</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">is_on_host</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code> / <code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p>Host vs device memory location of scaling factor</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">qmode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">static_sazp</span></code> , <code class="docutils literal notranslate"><span class="pre">dynamic_mx</span></code> , <code class="docutils literal notranslate"><span class="pre">dynamic_fp</span></code></p></td>
<td><p>Quantization mode: static with scales and zero-points, dynamic (MXFP8 compatible), dynamic (NVFP4 compatible)</p></td>
</tr>
</tbody>
</table>
</div>
<p>(*) Support for quantization options varies based on individual primitive and target hardware. Refer to primitives documentation for the details.</p>
</section>
<section id="supported-scaling-granularity-levels">
<h4>Supported Scaling Granularity Levels<a class="headerlink" href="#supported-scaling-granularity-levels" title="Permalink to this heading">#</a></h4>
<p>oneDNN supports the following scaling granularity levels to support different quantization schemes:</p>
<ul class="simple">
<li><p><a class="reference external" href="#per-tensor-scaling">Per-tensor scaling</a> (<code class="docutils literal notranslate"><span class="pre">mask=0</span></code>) uses a single scaling factor for the entire tensor, making it the simplest approach.</p></li>
<li><p><a class="reference external" href="#per-channel-scaling">Per-channel scaling</a> (<code class="docutils literal notranslate"><span class="pre">mask=1&lt;&lt;dim</span></code>) applies different scaling factors along a specific dimension, for instance commonly used for CNN weights.</p></li>
<li><p><a class="reference external" href="#block-scaling">Block scaling</a> subdivides tensor dimensions into smaller blocks with individual scaling factors, important for large transformer models and advanced quantization techniques.</p></li>
<li><p><a class="reference external" href="#multi-dimensional-scaling">Multi-dimensional scaling</a> (<code class="docutils literal notranslate"><span class="pre">mask=(1&lt;&lt;dim1)+(1&lt;&lt;dim2)</span></code>) provides independent scaling factors along multiple tensor dimensions, useful for complex activations where both batch and channel dimensions need separate scaling.</p></li>
</ul>
<section id="per-tensor-scaling">
<h5>Per-tensor Scaling<a class="headerlink" href="#per-tensor-scaling" title="Permalink to this heading">#</a></h5>
<p>In the simplest case, when there is only one common scaling factor the attribute changes the op behavior from</p>
<div class="math notranslate nohighlight">
\[\dst[:] = Op(...)\]</div>
<p>to</p>
<div class="math notranslate nohighlight">
\[\dst[:] = scale \cdot Op(...).\]</div>
<pre class="highlight literal-block"><span></span><span class="c1">// Using full set_scales API (recommended)</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a35152c75fb7d3e44aa7e51e9ac25c3b0"><span class="std std-ref">set_scales</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gac37ad67b48edeb9e742af0e50b70fe09"><span class="std std-ref">DNNL_ARG_SRC</span></a><span></span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{},</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">);</span>

<span class="c1">// Using convenience set_host_scale API for host-side scaling factor</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a7b035390cde177453afae9c5b5a7c29e"><span class="std std-ref">set_host_scale</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gac37ad67b48edeb9e742af0e50b70fe09"><span class="std std-ref">DNNL_ARG_SRC</span></a><span></span><span class="p">,</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">);</span>

<span class="c1">// Using legacy set_scales_mask API</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ac3dc9efa6702a5eba6f289f1b3907590"><span class="std std-ref">set_scales_mask</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gac37ad67b48edeb9e742af0e50b70fe09"><span class="std std-ref">DNNL_ARG_SRC</span></a><span></span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

<span class="c1">// Scaling factors: 1 value</span>
<span class="c1">// Usage: All elements use same scaling factor</span></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more details on global scaling with a single scaling factor residing on host, use <a class="reference internal" href="#doxid-dev-guide-attributes-quantization-1host-side-scalars-and-zero-points"><span class="std std-ref">host-side scalar scaling</span></a> (<code class="docutils literal notranslate"><span class="pre">set_host_scale</span></code>) to avoid device memory transfer overhead.</p>
</div>
<p>See examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="#convolution-with-per-output-channel-quantization">Convolution with Per-output-channel Quantization</a></p></li>
</ul>
</section>
<section id="per-channel-scaling">
<h5>Per-Channel Scaling<a class="headerlink" href="#per-channel-scaling" title="Permalink to this heading">#</a></h5>
<p>Per-channel scaling applies different scaling factors along specific tensor dimensions. For instance, it is commonly used for CNN weights where each output channel has its own scaling factor.</p>
<pre class="highlight literal-block"><span></span><span class="c1">// Scaling factor per output channel (dimension 0 of weights)</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a35152c75fb7d3e44aa7e51e9ac25c3b0"><span class="std std-ref">set_scales</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{},</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">);</span>

<span class="c1">// Tensor: [OC, IC, H, W] = [64, 128, 3, 3]</span>
<span class="c1">// Scaling factors: 64 values (one per output channel)</span>
<span class="c1">// Usage: Each output channel gets its own scaling factor</span></pre>
<p>See examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="#weights-preparation-with-per-output-channel-scaling">Weights Preparation with Per-output-channel Scaling</a></p></li>
<li><p><a class="reference external" href="#convolution-with-per-output-channel-quantization">Convolution with Per-output-channel Quantization</a></p></li>
<li><p><a class="reference internal" href="page_inference_int8_matmul_cpp.html#doxid-inference-int8-matmul-cpp"><span class="std std-ref">MatMul Tutorial: INT8 Inference</span></a></p></li>
</ul>
</section>
<section id="block-scaling">
<h5>Block Scaling<a class="headerlink" href="#block-scaling" title="Permalink to this heading">#</a></h5>
<p>Groups enable block-wise quantization by subdividing tensor dimensions into smaller blocks, each with its own scaling factor. This might help balance accuracy and efficiency by providing more granular quantization than per-tensor scaling.</p>
<pre class="highlight literal-block"><span></span><span class="c1">// Weight shape: [K, N] = [1024, 512] with groups [32, 1]</span>
<span class="c1">// Creates 32 groups along K dimension, each with its own scaling factor per N value</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">dim_t</span><span class="o">&gt;</span> <span class="n">groups</span> <span class="o">=</span> <span class="p">{</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">};</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a35152c75fb7d3e44aa7e51e9ac25c3b0"><span class="std std-ref">set_scales</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span>
                <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">);</span>

<span class="c1">// Tensor: [K, N] = [1024, 512]</span>
<span class="c1">// Scaling factors: 32  512 = 16,384 values (one per group)</span>
<span class="c1">// Usage: Each (group_k, n) combination gets its own scaling factor</span></pre>
<p>See examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="#matmul-with-advanced-quantization">Matmul with Advanced Quantization</a></p></li>
<li><p><a class="reference external" href="#matmul-with-precomputed-reductions-and-advanced-quantization">Matmul with Precomputed Reductions and Advanced Quantization</a></p></li>
<li><p><a class="reference internal" href="page_matmul_with_weight_only_quantization_cpp.html#doxid-matmul-with-weight-only-quantization-cpp"><span class="std std-ref">MatMul Tutorial: Weight-only Quantization</span></a></p></li>
</ul>
<section id="special-case-mx-compatible-block-scaling-or-dynamic-quantization">
<h6>Special Case: MX-compatible Block Scaling (or Dynamic Quantization)<a class="headerlink" href="#special-case-mx-compatible-block-scaling-or-dynamic-quantization" title="Permalink to this heading">#</a></h6>
<p>MX-compatible block scaling uses <code class="docutils literal notranslate"><span class="pre">e8m0</span></code> data type for scaling factors and <code class="docutils literal notranslate"><span class="pre">dynamic_mx</span></code> quantization mode to align with the <a class="reference external" href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">OCP MX Formats Specification</a>.</p>
<pre class="highlight literal-block"><span></span><span class="c1">// Set MX-compatible block scaling for weights</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a35152c75fb7d3e44aa7e51e9ac25c3b0"><span class="std std-ref">set_scales</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{</span><span class="mi">32</span><span class="p">},</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea8af1e244959fd40c752655b5d39980eb"><span class="std std-ref">dnnl::memory::data_type::e8m0</span></a><span></span><span class="p">,</span>
                <span class="nb">false</span> <span class="cm">/*on device*/</span><span class="p">,</span> <a class="reference internal" href="enum_dnnl_quantization_mode.html#doxid-group-dnnl-api-attributes-1gga43df4b809a4544d34bbc106d3e409b2ca10dabb84b08ade6e41ee83eba1e96f9d"><span class="std std-ref">dnnl::quantization_mode::dynamic_mx</span></a><span></span><span class="p">);</span>

<span class="c1">// Tensor: [K, N] = [1024, 512]</span>
<span class="c1">// Scaling factors: 32 values (one per group of 32 in K dimension)</span>
<span class="c1">// Usage: Each group of 32 in K dimension gets its own scaling factor</span></pre>
<p>See example <a class="reference internal" href="page_mxfp_matmul_cpp.html#doxid-mxfp-matmul-cpp"><span class="std std-ref">MatMul Tutorial: MXFP8 Inference</span></a>.</p>
</section>
</section>
<section id="multi-dimensional-scaling">
<h5>Multi-Dimensional Scaling<a class="headerlink" href="#multi-dimensional-scaling" title="Permalink to this heading">#</a></h5>
<p>Multi-dimensional scaling applies scaling factors across multiple tensor dimensions simultaneously.</p>
<p>For scaling factors per dimensions <span class="math notranslate nohighlight">\(d_i\)</span>, set <code class="docutils literal notranslate"><span class="pre">mask</span> <span class="pre">=</span></code> <span class="math notranslate nohighlight">\(\sum_{d_i} 2^{d_i}\)</span>.</p>
<p>Resulting scaling factor count without groups: <span class="math notranslate nohighlight">\(\prod_{d_i} D_{d_i}\)</span>, with groups: <span class="math notranslate nohighlight">\(\prod_{d_i} G_{d_i}\)</span>.</p>
<pre class="highlight literal-block"><span></span><span class="c1">// Scaling factors vary along batch and channel dimensions</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a35152c75fb7d3e44aa7e51e9ac25c3b0"><span class="std std-ref">set_scales</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gac37ad67b48edeb9e742af0e50b70fe09"><span class="std std-ref">DNNL_ARG_SRC</span></a><span></span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">),</span> <span class="p">{},</span>
                <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">,</span> <span class="nb">false</span><span class="p">);</span>

<span class="c1">// Tensor: [N, C, H, W] = [8, 64, 32, 32]</span>
<span class="c1">// Scaling factors needed: 8 * 64 = 512 values</span>
<span class="c1">// Usage: Each (batch, channel) combination gets its own scaling factor</span></pre>
<p>See examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="#matmul-with-advanced-quantization">Matmul with Advanced Quantization</a></p></li>
<li><p><a class="reference external" href="#matmul-with-precomputed-reductions-and-advanced-quantization">Matmul with Precomputed Reductions and Advanced Quantization</a></p></li>
<li><p><a class="reference internal" href="page_matmul_with_weight_only_quantization_cpp.html#doxid-matmul-with-weight-only-quantization-cpp"><span class="std std-ref">MatMul Tutorial: Weight-only Quantization</span></a></p></li>
</ul>
<p><span class="target" id="doxid-dev-guide-attributes-quantization-1dgaq-zps"></span></p>
</section>
</section>
</section>
<section id="argument-zero-points">
<h3>Argument Zero-Points<a class="headerlink" href="#argument-zero-points" title="Permalink to this heading">#</a></h3>
<p>Zero-points handle the quantization case where the quantized integer range does not center around zero.</p>
<p>The library uses <a class="reference internal" href="dev_guide_attributes.html#doxid-dev-guide-attributes"><span class="std std-ref">Primitive Attributes</span></a> API for setting zero-points for most primitives. The supporting attributes can be found in the documentation for each primitive. The unsupported cases are handled according to the <a class="reference internal" href="dev_guide_attributes.html#doxid-dev-guide-attributes-1dev-guide-attributes-error-handling"><span class="std std-ref">attributes error handling section</span></a>.</p>
<section id="available-zero-point-api-methods">
<h4>Available Zero-Point API Methods<a class="headerlink" href="#available-zero-point-api-methods" title="Permalink to this heading">#</a></h4>
<p>oneDNN provides the following methods for setting zero-points:</p>
<pre class="highlight literal-block"><span></span><span class="c1">// Legacy method with simple mask-based zero-points</span>
<span class="kt">void</span> <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a8935d36d48fe5db9476b30b02791d822"><span class="std std-ref">dnnl::primitive_attr::set_zero_points_mask</span></a><span></span><span class="p">(</span><span class="kt">int</span> <span class="n">arg</span><span class="p">,</span> <span class="kt">int</span> <span class="n">mask</span><span class="p">);</span>

<span class="c1">// Generic method with groups support</span>
<span class="kt">void</span> <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a2a8693f2aba0541ccd59470b41321175"><span class="std std-ref">dnnl::primitive_attr::set_zero_points</span></a><span></span><span class="p">(</span><span class="kt">int</span> <span class="n">arg</span><span class="p">,</span> <span class="kt">int</span> <span class="n">mask</span><span class="p">,</span>
                                          <span class="k">const</span> <a class="reference internal" href="struct_dnnl_memory-2.html#doxid-structdnnl-1-1memory-1a7d9f4b6ad8caf3969f436cd9ff27e9bb"><span class="std std-ref">dnnl::memory::dims</span></a><span></span> <span class="o">&amp;</span><span class="n">groups</span><span class="p">,</span>
                                          <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dce"><span class="std std-ref">dnnl::memory::data_type</span></a><span></span> <span class="n">data_type</span> <span class="o">=</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dceaa860868d23f3a68323a2e3f6563d7f31"><span class="std std-ref">dnnl::memory::data_type::s32</span></a><span></span><span class="p">,</span>
                                          <span class="kt">bool</span> <span class="n">is_on_host</span> <span class="o">=</span> <span class="nb">false</span><span class="p">);</span>

<span class="c1">// Convenience method for single host-side scalar</span>
<span class="kt">void</span> <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ac6aac2aa4418da036964baa3a35ed879"><span class="std std-ref">dnnl::primitive_attr::set_host_zero_point</span></a><span></span><span class="p">(</span><span class="kt">int</span> <span class="n">arg</span><span class="p">,</span>
                                              <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dce"><span class="std std-ref">dnnl::memory::data_type</span></a><span></span> <span class="n">data_type</span> <span class="o">=</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dceaa860868d23f3a68323a2e3f6563d7f31"><span class="std std-ref">dnnl::memory::data_type::s32</span></a><span></span><span class="p">);</span></pre>
<p>Key parameters of the zero-point API methods are summarized below:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Options*</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">arg</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DNNL_ARG_SRC</span></code> , <code class="docutils literal notranslate"><span class="pre">DNNL_ARG_WEIGHTS</span></code> , <code class="docutils literal notranslate"><span class="pre">DNNL_ARG_DST</span></code></p></td>
<td><p>Tensor to apply zero-point</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mask</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code> , <code class="docutils literal notranslate"><span class="pre">1&lt;&lt;dim</span></code> , <code class="docutils literal notranslate"><span class="pre">(1&lt;&lt;d1)+(1&lt;&lt;d2)</span></code></p></td>
<td><p>Zero-point granularity: global, per-dimension, multi-dimensional</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">groups</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code> , <code class="docutils literal notranslate"><span class="pre">{G}</span></code> , <code class="docutils literal notranslate"><span class="pre">{G1,G2,...}</span></code></p></td>
<td><p>Block quantization: none, single-size, multi-dimensional blocks</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">data_type</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">s32</span></code> , <code class="docutils literal notranslate"><span class="pre">s8</span></code> , <code class="docutils literal notranslate"><span class="pre">u8</span></code> , <code class="docutils literal notranslate"><span class="pre">s4</span></code> , <code class="docutils literal notranslate"><span class="pre">u4</span></code></p></td>
<td><p>Zero-point data type</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">is_on_host</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code> / <code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p>Host vs device memory location of zero-point</p></td>
</tr>
</tbody>
</table>
</div>
<p>(*) Support for quantization options varies based on individual primitive and target hardware. Refer to primitives documentation for the details.</p>
</section>
<section id="supported-zero-point-granularity-levels">
<h4>Supported Zero-Point Granularity Levels<a class="headerlink" href="#supported-zero-point-granularity-levels" title="Permalink to this heading">#</a></h4>
<p>Zero-point granularity mirrors the scaling factor granularity described above. The same mask and groups concepts apply:</p>
<ul class="simple">
<li><p>Per-tensor zero-point (<code class="docutils literal notranslate"><span class="pre">mask=0</span></code>): Single zero-point for entire tensor</p></li>
<li><p>Per-channel zero-points (<code class="docutils literal notranslate"><span class="pre">mask=1&lt;&lt;dim</span></code>): Different zero-points per channel</p></li>
<li><p>Block zero-points (<code class="docutils literal notranslate"><span class="pre">mask</span></code> with <code class="docutils literal notranslate"><span class="pre">groups</span></code>): Block-wise zero-points</p></li>
<li><p>Multi-dimensional zero-points (<code class="docutils literal notranslate"><span class="pre">mask=(1&lt;&lt;dim1)+(1&lt;&lt;dim2)</span></code>): Independent zero-points across multiple dimensions</p></li>
</ul>
<pre class="highlight literal-block"><span></span><span class="c1">// Per-tensor zero-point</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a2a8693f2aba0541ccd59470b41321175"><span class="std std-ref">set_zero_points</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gac37ad67b48edeb9e742af0e50b70fe09"><span class="std std-ref">DNNL_ARG_SRC</span></a><span></span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{},</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dceaa860868d23f3a68323a2e3f6563d7f31"><span class="std std-ref">dnnl::memory::data_type::s32</span></a><span></span><span class="p">);</span>

<span class="c1">// Per-channel zero-points</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a2a8693f2aba0541ccd59470b41321175"><span class="std std-ref">set_zero_points</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{},</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea3e8d88fdd85d7153525e0647cdd97686"><span class="std std-ref">dnnl::memory::data_type::s8</span></a><span></span><span class="p">);</span>

<span class="c1">// Block zero-points</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">dnnl</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">dim_t</span><span class="o">&gt;</span> <span class="n">groups</span> <span class="o">=</span> <span class="p">{</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">};</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a2a8693f2aba0541ccd59470b41321175"><span class="std std-ref">set_zero_points</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span>
                     <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dceaa860868d23f3a68323a2e3f6563d7f31"><span class="std std-ref">dnnl::memory::data_type::s32</span></a><span></span><span class="p">);</span></pre>
<p>See examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="#convolution-with-per-output-channel-quantization">Convolution with Per-output-channel Quantization</a></p></li>
<li><p><a class="reference external" href="#matmul-with-precomputed-reductions-and-advanced-quantization">Matmul with Precomputed Reductions and Advanced Quantization</a></p></li>
<li><p><a class="reference internal" href="page_inference_int8_matmul_cpp.html#doxid-inference-int8-matmul-cpp"><span class="std std-ref">MatMul Tutorial: INT8 Inference</span></a></p></li>
<li><p><a class="reference internal" href="page_matmul_with_weight_only_quantization_cpp.html#doxid-matmul-with-weight-only-quantization-cpp"><span class="std std-ref">MatMul Tutorial: Weight-only Quantization</span></a></p></li>
</ul>
<p><span class="target" id="doxid-dev-guide-attributes-quantization-1host-side-scalars-and-zero-points"></span></p>
</section>
</section>
<section id="special-case-host-side-scalar-scaling-factor-and-zero-point">
<h3>Special Case: Host-side Scalar Scaling Factor and Zero-point<a class="headerlink" href="#special-case-host-side-scalar-scaling-factor-and-zero-point" title="Permalink to this heading">#</a></h3>
<p>When using the GPU engine and per-tensor quantization, host-side scaling factor and zero-point are supported to reduce copying of data from host to device. A memory object for scaling factor or zero-point value should be created as a host-side scalar (see <a class="reference internal" href="dev_guide_host_side_scalars.html#doxid-dev-guide-host-side-scalars"><span class="std std-ref">Host-Side Scalars Support</span></a> for details) and passed to the primitive execution function.</p>
<p>The host scaling factor or zero-point attributes could also be set using the following convenience API:</p>
<pre class="highlight literal-block"><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr"><span class="std std-ref">dnnl::primitive_attr</span></a><span></span> <span class="n">attr</span><span class="p">;</span>
<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a7b035390cde177453afae9c5b5a7c29e"><span class="std std-ref">set_host_scale</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1ga3ca217e4a06d42a0ede3c018383c388f"><span class="std std-ref">DNNL_ARG_DST</span></a><span></span><span class="p">,</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">);</span>

<span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ac6aac2aa4418da036964baa3a35ed879"><span class="std std-ref">set_host_zero_point</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1ga3ca217e4a06d42a0ede3c018383c388f"><span class="std std-ref">DNNL_ARG_DST</span></a><span></span><span class="p">,</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dceaa860868d23f3a68323a2e3f6563d7f31"><span class="std std-ref">dnnl::memory::data_type::s32</span></a><span></span><span class="p">);</span></pre>
<p>See examples:</p>
<ul class="simple">
<li><p><a class="reference internal" href="page_matmul_with_host_scalar_scale_cpp.html#doxid-matmul-with-host-scalar-scale-cpp"><span class="std std-ref">MatMul with Host Scalar Scale example</span></a></p></li>
</ul>
<p><span class="target" id="doxid-dev-guide-attributes-quantization-1dgaq-precomputed-reductions"></span></p>
</section>
<section id="precomputed-reductions">
<h3>Precomputed Reductions<a class="headerlink" href="#precomputed-reductions" title="Permalink to this heading">#</a></h3>
<p>Precomputed reductions could help optimize performance for Large Language Models (LLM).</p>
<p>When using block-wise zero-points for quantized weights, the library must compute reductions over the source tensor during matrix multiplication. This involves summing source tensor values across groups along the reduction dimension:</p>
<div class="math notranslate nohighlight">
\[\dst_{m,n}=\sum_{g=0}^{G-1}\sum_{k={K\over{G}}g}^{{K\over{G}}(g+1)-1}{\src_{m,k}(\weights_{k,n}-zp_{\weights}(g,n))}=\sum_{k=0}^{K-1}{\src_{m,k}\weights_{k,n}}-\sum_{g=0}^{G-1}zp_{\weights}(g,n)\underbrace{\sum_{k={K\over{G}}g}^{{K\over{G}}(g+1)-1}\src_{m,k}}_{R_{m,g}}\]</div>
<p>where <code class="docutils literal notranslate"><span class="pre">R</span></code> represents the precomputed reductions that can be calculated externally when quantizing the source tensor, therefore removing the need for the library to compute them at runtime.</p>
<p>The library uses <a class="reference internal" href="dev_guide_attributes.html#doxid-dev-guide-attributes"><span class="std std-ref">Primitive Attributes</span></a> API for setting precomputed reductions. The supporting attributes can be found in the documentation for each primitive. The unsupported cases are handled according to the <a class="reference internal" href="dev_guide_attributes.html#doxid-dev-guide-attributes-1dev-guide-attributes-error-handling"><span class="std std-ref">attributes error handling section</span></a>.</p>
<section id="available-precomputed-reductions-api-method">
<h4>Available Precomputed Reductions API Method<a class="headerlink" href="#available-precomputed-reductions-api-method" title="Permalink to this heading">#</a></h4>
<p>oneDNN provides the following method for setting precomputed reductions:</p>
<pre class="highlight literal-block"><span></span><span class="kt">void</span> <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a24a349d345ac97756a54b01b634b1b3c"><span class="std std-ref">dnnl::primitive_attr::set_precomputed_reductions</span></a><span></span><span class="p">(</span><span class="kt">int</span> <span class="n">arg</span><span class="p">,</span> <span class="kt">int</span> <span class="n">mask</span><span class="p">,</span>
        <span class="k">const</span> <a class="reference internal" href="struct_dnnl_memory-2.html#doxid-structdnnl-1-1memory-1a7d9f4b6ad8caf3969f436cd9ff27e9bb"><span class="std std-ref">dnnl::memory::dims</span></a><span></span> <span class="o">&amp;</span><span class="n">groups</span><span class="p">,</span>
        <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dce"><span class="std std-ref">dnnl::memory::data_type</span></a><span></span> <span class="n">data_type</span> <span class="o">=</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dceaa860868d23f3a68323a2e3f6563d7f31"><span class="std std-ref">dnnl::memory::data_type::s32</span></a><span></span><span class="p">);</span></pre>
<p>Key parameters of the precomputed reductions API method are summarized below:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Options*</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">arg</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DNNL_ARG_SRC</span></code></p></td>
<td><p>Tensor to apply precomputed reductions</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mask</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code> , <code class="docutils literal notranslate"><span class="pre">1&lt;&lt;dim</span></code> , <code class="docutils literal notranslate"><span class="pre">(1&lt;&lt;d1)+(1&lt;&lt;d2)</span></code></p></td>
<td><p>Reduction granularity: global, per-dimension, multi-dimensional</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">groups</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code> , <code class="docutils literal notranslate"><span class="pre">{G}</span></code> , <code class="docutils literal notranslate"><span class="pre">{G1,G2,...}</span></code></p></td>
<td><p>Block quantization: none, single-size, multi-dimensional blocks</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">data_type</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">s32</span></code></p></td>
<td><p>Reduction data type</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following limitations apply when using precomputed reductions:</p>
<ul class="simple">
<li><p>Requires weight zero-points: Cannot be used without weights zero-points specified.</p></li>
<li><p>Full matrix mask required: Must have full A matrix mask, meaning broadcast is not supported.</p></li>
</ul>
</div>
<p>(*) Support for quantization options varies based on individual primitive and target hardware. Refer to primitives documentation for the details.</p>
<p>See examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="#matrix-multiplication-with-precomputed-reductions-and-advanced-quantization">Matmul with Precomputed Reductions and Advanced Quantization</a></p></li>
</ul>
</section>
</section>
</section>
<section id="quantization-workflows-examples">
<h2>Quantization Workflows Examples<a class="headerlink" href="#quantization-workflows-examples" title="Permalink to this heading">#</a></h2>
<section id="breakdown-of-convolution-with-int8-quantization">
<h3>Breakdown of Convolution with INT8 Quantization<a class="headerlink" href="#breakdown-of-convolution-with-int8-quantization" title="Permalink to this heading">#</a></h3>
<p>Consider a convolution with bias. The tensors are represented as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\src_{f32}[:] = scale_{\src} \cdot (\src_{int8}[:] - zp_{\src})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\weights_{f32}[:] = scale_{\weights} \cdot \weights_{int8}[:]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dst_{f32}[:] = scale_{\dst} \cdot (\dst_{int8}[:] - zp_{\dst})\)</span></p></li>
</ul>
<p>Here the <span class="math notranslate nohighlight">\(\src_{f32}, \weights_{f32}, \dst_{f32}\)</span> are not computed at all, the whole work happens with int8 tensors. So the task is to compute the <span class="math notranslate nohighlight">\(\dst_{int8}\)</span> tensor, using the <span class="math notranslate nohighlight">\(\src_{int8}\)</span>, <span class="math notranslate nohighlight">\(\weights_{int8}\)</span> tensors passed at execution time, as well as the corresponding quantization parameters <span class="math notranslate nohighlight">\(scale_{\src}\)</span>, <span class="math notranslate nohighlight">\(scale_{\weights}\)</span>, <span class="math notranslate nohighlight">\(scale_{\dst}\)</span>, and <span class="math notranslate nohighlight">\(zp_{\src}\)</span>, <span class="math notranslate nohighlight">\(zp_{\dst}\)</span>. Mathematically, the computations are:</p>
<div class="math notranslate nohighlight">
\[\dst_{int8}[:] = \operatorname{f32\_to\_int8}( (scale_{\src} \cdot scale_{\weights} \cdot \operatorname{s32\_to\_f32}(conv_{s32}(\src_{int8}, \weights_{int8}) - zp_{\src} \cdot comp_{s32}) + bias_{f32}) / scale_{\dst} + zp_{\dst} )\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\operatorname{conv}_{s32}\)</span> is just a regular convolution which takes source and weights with int8 data type and compute the result in int32 data type (int32 is chosen to avoid overflows during the computations);</p></li>
<li><p><span class="math notranslate nohighlight">\(comp_{s32}\)</span> is a compensation term to account for <span class="math notranslate nohighlight">\(\src\)</span> non-zero zero-point. This term is computed by the oneDNN library and can typically be pre-computed ahead of time, for example during weights reorder.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{f32\_to\_s8}()\)</span> converts an <code class="docutils literal notranslate"><span class="pre">f32</span></code> value to <code class="docutils literal notranslate"><span class="pre">s8</span></code> with potential saturation if the values are out of the range of the int8 data type.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{s32\_to\_f32}()\)</span> converts an <code class="docutils literal notranslate"><span class="pre">int8</span></code> value to <code class="docutils literal notranslate"><span class="pre">f32</span></code> with potential rounding. This conversion is typically necessary to apply <code class="docutils literal notranslate"><span class="pre">f32</span></code> scaling factors.</p></li>
</ul>
<section id="per-channel-scaling-specifics">
<h4>Per-Channel Scaling Specifics<a class="headerlink" href="#per-channel-scaling-specifics" title="Permalink to this heading">#</a></h4>
<p>Some of the primitives have limited support of multiple scales for a quantized tensor. The most popular use case is the <a class="reference internal" href="dev_guide_convolution.html#doxid-dev-guide-convolution"><span class="std std-ref">Convolution</span></a> primitive that supports per-output-channel scaling factors for the weights, meaning that the actual convolution computations would need to scale different output channels differently. This is possible without significant performance loss because the per-output-channel re-quantization is only required at the very end of the computations. It seems impossible to implement the same trick for the input channels, since that would require re-quantization for every input data point.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\src_{f32}(n, ic, ih, iw) = scale_{\src} \cdot \src_{int8}(n, ic, ih, iw)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\weights_{f32}(oc, ic, kh, kw) = scale_{\weights}(oc) \cdot \weights_{int8}(oc, ic, kh, kw)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dst_{f32}(n, oc, oh, ow) = scale_{\dst} \cdot \dst_{int8}(n, oc, oh, ow)\)</span></p></li>
</ul>
<p>Note that now the weights scaling factor depends on <span class="math notranslate nohighlight">\(oc\)</span>.</p>
<p>To compute the <span class="math notranslate nohighlight">\(\dst_{int8}\)</span> we need to perform the following:</p>
<div class="math notranslate nohighlight">
\[\dst_{int8}(n, oc, oh, ow) = \operatorname{f32\_to\_int8}( \frac{scale_{\src} \cdot scale_{\weights}(oc) \cdot conv_{s32}(\src_{int8}, \weights_{int8})|_{(n, oc, oh, ow)} + \bias_{f32}}{scale_{\dst}} ).\]</div>
<p>The user is responsible for preparing quantized weights accordingly. To do that, oneDNN provides reorders that can perform per-channel scaling:</p>
<div class="math notranslate nohighlight">
\[\weights_{int8}(oc, ic, kh, kw) = \operatorname{f32\_to\_int8}( \weights_{f32}(oc, ic, kh, kw) / scale_{weights}(oc) ).\]</div>
</section>
<section id="weights-preparation-with-per-output-channel-scaling">
<h4>Weights Preparation with Per-output-channel Scaling<a class="headerlink" href="#weights-preparation-with-per-output-channel-scaling" title="Permalink to this heading">#</a></h4>
<pre class="highlight literal-block"><span></span>   <span class="c1">// weights dimensions</span>
   <span class="k">const</span> <span class="kt">int</span> <span class="n">OC</span><span class="p">,</span> <span class="n">IC</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">;</span>

   <span class="c1">// original f32 weights in plain format</span>
   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">wei_plain_f32_md</span><span class="p">(</span>
           <span class="p">{</span><span class="n">OC</span><span class="p">,</span> <span class="n">IC</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">},</span>                 <span class="c1">// dims</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e"><span class="std std-ref">dnnl::memory::data_type::f32</span></a><span></span><span class="p">,</span>     <span class="c1">// the data originally in f32</span>
           <a class="reference internal" href="enum_dnnl_memory_format_tag.html#doxid-structdnnl-1-1memory-1a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fafd710c828421b3c91725b0e5aa53ecc6"><span class="std std-ref">dnnl::memory::format_tag::hwigo</span></a><span></span>   <span class="c1">// the plain memory format</span>
           <span class="p">);</span>

   <span class="c1">// the scaling factors for quantized weights</span>
   <span class="c1">// An unique scale for each output-channel.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">OC</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span> <span class="cm">/* values */</span> <span class="p">};</span>
   <a class="reference internal" href="struct_dnnl_memory-2.html#doxid-structdnnl-1-1memory"><span class="std std-ref">dnnl::memory</span></a><span></span><span class="p">();</span>

   <span class="c1">// int8 convolution primitive descriptor</span>
   <a class="reference internal" href="struct_dnnl_convolution_forward_primitive_desc.html#doxid-structdnnl-1-1convolution-forward-1-1primitive-desc"><span class="std std-ref">dnnl::convolution_forward::primitive_desc</span></a><span></span> <span class="n">conv_pd</span><span class="p">(</span><span class="cm">/* see the convolution workflow section */</span><span class="p">);</span>

   <span class="c1">// query the convolution weights memory descriptor</span>
   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">wei_conv_s8_md</span> <span class="o">=</span> <span class="n">conv_pd</span><span class="p">.</span><span class="n">weights_desc</span><span class="p">();</span>

   <span class="c1">// prepare the attributes for the reorder</span>
   <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr"><span class="std std-ref">dnnl::primitive_attr</span></a><span></span> <span class="n">attr</span><span class="p">;</span>
   <span class="k">const</span> <span class="kt">int</span> <span class="n">quantization_mask</span> <span class="o">=</span> <span class="mi">0</span>
       <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">);</span>  <span class="c1">// scale per  OC dimension, which is the dim #0</span>
   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ac3dc9efa6702a5eba6f289f1b3907590"><span class="std std-ref">set_scales_mask</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1ga3ca217e4a06d42a0ede3c018383c388f"><span class="std std-ref">DNNL_ARG_DST</span></a><span></span><span class="p">,</span> <span class="n">quantization_mask</span><span class="p">);</span>

   <span class="c1">// create reorder that would perform:</span>
   <span class="c1">//   wei_s8(oc, ic, kh, kw) &lt;- wei_f32(oc, ic, kh, kw) / scale(oc)</span>
   <span class="c1">// including the data format conversion.</span>
   <span class="k">auto</span> <span class="n">wei_reorder_pd</span> <span class="o">=</span> <a class="reference internal" href="struct_dnnl_reorder_primitive_desc.html#doxid-structdnnl-1-1reorder-1-1primitive-desc"><span class="std std-ref">dnnl::reorder::primitive_desc</span></a><span></span><span class="p">(</span>
           <span class="n">wei_plain_f32_md</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> <span class="c1">// source</span>
           <span class="n">wei_conv_s8_md</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> <span class="c1">// destination,</span>
           <span class="n">attr</span><span class="p">);</span>
   <span class="k">auto</span> <span class="n">wei_reorder</span> <span class="o">=</span> <a class="reference internal" href="struct_dnnl_reorder.html#doxid-structdnnl-1-1reorder"><span class="std std-ref">dnnl::reorder</span></a><span></span><span class="p">(</span><span class="n">wei_reorder_pd</span><span class="p">);</span>

<span class="c1">// ...</span></pre>
</section>
<section id="convolution-with-per-output-channel-quantization">
<h4>Convolution with Per-output-channel Quantization<a class="headerlink" href="#convolution-with-per-output-channel-quantization" title="Permalink to this heading">#</a></h4>
<p>Building upon the weights preparation shown above, this section shows the complete workflow for an int8 convolution that combines per-output-channel weight scaling with global source and destination scaling.</p>
<pre class="highlight literal-block"><span></span>   <span class="k">const</span> <span class="kt">float</span> <span class="n">src_scale</span><span class="p">;</span> <span class="c1">// src_f32[:] = src_scale * src_s8[:]</span>
   <span class="k">const</span> <span class="kt">float</span> <span class="n">dst_scale</span><span class="p">;</span> <span class="c1">// dst_f32[:] = dst_scale * dst_s8[:]</span>

   <span class="c1">// the scaling factors for quantized weights (as declared above)</span>
   <span class="c1">// An unique scale for each output-channel.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">OC</span><span class="p">)</span> <span class="o">=</span> <span class="p">{...};</span>


   <span class="c1">// Src, weights, and dst memory descriptors for convolution,</span>
   <span class="c1">// with memory format tag == any to allow a convolution implementation</span>
   <span class="c1">// to chose the appropriate memory format</span>

   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">src_conv_s8_any_md</span><span class="p">(</span>
           <span class="p">{</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">IC</span><span class="p">,</span> <span class="n">IH</span><span class="p">,</span> <span class="n">IW</span><span class="p">},</span>          <span class="c1">// dims</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea3e8d88fdd85d7153525e0647cdd97686"><span class="std std-ref">dnnl::memory::data_type::s8</span></a><span></span><span class="p">,</span>  <span class="c1">// the data originally in s8</span>
           <a class="reference internal" href="enum_dnnl_memory_format_tag.html#doxid-structdnnl-1-1memory-1a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec"><span class="std std-ref">dnnl::memory::format_tag::any</span></a><span></span> <span class="c1">// let convolution to choose</span>
           <span class="p">);</span>

   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">wei_conv_s8_any_md</span><span class="p">(</span>
           <span class="p">{</span><span class="n">OC</span><span class="p">,</span> <span class="n">IC</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span><span class="p">},</span>             <span class="c1">// dims</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea3e8d88fdd85d7153525e0647cdd97686"><span class="std std-ref">dnnl::memory::data_type::s8</span></a><span></span><span class="p">,</span>  <span class="c1">// the data originally in s8</span>
           <a class="reference internal" href="enum_dnnl_memory_format_tag.html#doxid-structdnnl-1-1memory-1a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec"><span class="std std-ref">dnnl::memory::format_tag::any</span></a><span></span> <span class="c1">// let convolution to choose</span>
           <span class="p">);</span>

   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">dst_conv_s8_any_md</span><span class="p">(...);</span>  <span class="c1">// ditto</span>

   <span class="c1">// prepare the attributes for the convolution</span>
   <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr"><span class="std std-ref">dnnl::primitive_attr</span></a><span></span> <span class="n">attr</span><span class="p">;</span>
   <span class="k">const</span> <span class="kt">int</span> <span class="n">data_mask</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">// scale and zero-point per tensor for source and destination</span>
   <span class="k">const</span> <span class="kt">int</span> <span class="n">wei_mask</span> <span class="o">=</span> <span class="mi">0</span>
       <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">);</span> <span class="c1">// scale per OC dimension, which is the dim #0 on weights tensor:</span>
                   <span class="c1">// (   OC, IC, KH, KW)</span>
                   <span class="c1">//      0   1   2   3</span>

   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ac3dc9efa6702a5eba6f289f1b3907590"><span class="std std-ref">set_scales_mask</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gac37ad67b48edeb9e742af0e50b70fe09"><span class="std std-ref">DNNL_ARG_SRC</span></a><span></span><span class="p">,</span> <span class="n">data_mask</span><span class="p">);</span>
   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a8935d36d48fe5db9476b30b02791d822"><span class="std std-ref">set_zero_points_mask</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gac37ad67b48edeb9e742af0e50b70fe09"><span class="std std-ref">DNNL_ARG_SRC</span></a><span></span><span class="p">,</span> <span class="n">data_mask</span><span class="p">);</span>

   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ac3dc9efa6702a5eba6f289f1b3907590"><span class="std std-ref">set_scales_mask</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="n">wei_mask</span><span class="p">);</span>

   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ac3dc9efa6702a5eba6f289f1b3907590"><span class="std std-ref">set_scales_mask</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1ga3ca217e4a06d42a0ede3c018383c388f"><span class="std std-ref">DNNL_ARG_DST</span></a><span></span><span class="p">,</span> <span class="n">data_mask</span><span class="p">);</span>
   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a8935d36d48fe5db9476b30b02791d822"><span class="std std-ref">set_zero_points_mask</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1ga3ca217e4a06d42a0ede3c018383c388f"><span class="std std-ref">DNNL_ARG_DST</span></a><span></span><span class="p">,</span> <span class="n">data_mask</span><span class="p">);</span>

   <span class="c1">// create a convolution primitive descriptor</span>
   <span class="k">auto</span> <span class="n">conv_pd</span> <span class="o">=</span> <a class="reference internal" href="struct_dnnl_convolution_forward_primitive_desc.html#doxid-structdnnl-1-1convolution-forward-1-1primitive-desc"><span class="std std-ref">dnnl::convolution_forward::primitive_desc</span></a><span></span><span class="p">(</span>
           <a class="reference internal" href="enum_dnnl_prop_kind.html#doxid-group-dnnl-api-attributes-1ggac7db48f6583aa9903e54c2a39d65438fa3b9fad4f80d45368f856b5403198ac4c"><span class="std std-ref">dnnl::prop_kind::forward_inference</span></a><span></span><span class="p">,</span>
           <a class="reference internal" href="enum_dnnl_algorithm.html#doxid-group-dnnl-api-attributes-1gga00377dd4982333e42e8ae1d09a309640a5028ad8f818a45333a8a0eefad35c5c0"><span class="std std-ref">dnnl::algorithm::convolution_direct</span></a><span></span><span class="p">,</span>
           <span class="n">src_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// what&#39;s important is that</span>
           <span class="n">wei_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// we specified that we want</span>
           <span class="n">dst_conv_s8_any_md</span><span class="p">,</span>                     <span class="c1">// computations in s8</span>
           <span class="n">strides</span><span class="p">,</span> <span class="n">padding_l</span><span class="p">,</span> <span class="n">padding_r</span><span class="p">,</span>
           <span class="n">dnnl</span><span class="o">::</span><span class="n">padding_kind</span><span class="o">::</span><span class="n">zero</span>
           <span class="n">attr</span><span class="p">);</span>   <span class="c1">// the attributes describe the quantization flow</span>
<span class="c1">// ...</span></pre>
</section>
</section>
<section id="matrix-multiplication-with-weight-only-quantization-woq">
<h3>Matrix Multiplication with Weight-only Quantization (WoQ)<a class="headerlink" href="#matrix-multiplication-with-weight-only-quantization-woq" title="Permalink to this heading">#</a></h3>
<p>This example describes a process of weight-only quantization (WoQ) in matmul primitive which may be found when running Large Language Models (LLM). The advanced quantization here implies additional grouping introduced over reduction dimension besides traditional per-N quantization.</p>
<p>Weight-only quantization (WoQ) is the runtime process of converting integer weights back to floating-point format during computations. The primitive dequantizes weights using provided scales and zero-points, and converts them to the computation precision specified by <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ab00639157a283596834ee5b0e8478a2d"><span class="std std-ref">dnnl::primitive_attr::set_fpmath_mode</span></a>. See <a class="reference internal" href="dev_guide_attributes_fpmath_mode.html#doxid-dev-guide-attributes-fpmath-mode"><span class="std std-ref">Floating-point Math Mode</span></a> for details, and the code snippet below for an example of setting fpmath mode. For a full tutorial, refer to <a class="reference internal" href="page_matmul_with_weight_only_quantization_cpp.html#doxid-matmul-with-weight-only-quantization-cpp"><span class="std std-ref">MatMul Tutorial: Weight-only Quantization</span></a>.</p>
<pre class="highlight literal-block"><span></span>   <span class="c1">// Src, weights, and dst memory descriptors for matmul.</span>
   <span class="c1">// Consider simple 2D matmul case.</span>
   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">src_f16_any_md</span><span class="p">(...);</span>
   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">wei_s8_any_md</span><span class="p">(</span>
           <span class="p">{</span><span class="n">K</span> <span class="p">(</span><span class="mi">256</span><span class="p">),</span> <span class="n">N</span> <span class="p">(</span><span class="mi">512</span><span class="p">)},</span>           <span class="c1">// dims</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea3e8d88fdd85d7153525e0647cdd97686"><span class="std std-ref">dnnl::memory::data_type::s8</span></a><span></span><span class="p">,</span>  <span class="c1">// the data originally in s8</span>
           <a class="reference internal" href="enum_dnnl_memory_format_tag.html#doxid-structdnnl-1-1memory-1a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec"><span class="std std-ref">dnnl::memory::format_tag::any</span></a><span></span> <span class="c1">// let matmul to choose</span>
           <span class="p">);</span>
   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">dst_f16_any_md</span><span class="p">(...);</span>

   <span class="c1">// prepare the attributes</span>
   <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr"><span class="std std-ref">dnnl::primitive_attr</span></a><span></span> <span class="n">attr</span><span class="p">;</span>
   <span class="c1">// scale per K and N dimensions:</span>
   <span class="k">const</span> <span class="kt">int</span> <span class="n">wei_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">);</span>
   <span class="c1">// K dimension specifies the group size of `128`. It means that each 128</span>
   <span class="c1">// elements over K dimension will share a single value. For a given example,</span>
   <span class="c1">// there will be two groups, thus, two values referring to a single N value.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">dim_t</span><span class="o">&gt;</span> <span class="n">wei_groups</span> <span class="o">=</span> <span class="p">{</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">}</span>

   <span class="c1">// the scaling factors for quantized weights (as declared above)</span>
   <span class="c1">// A unique scale for each gK (256 / 128 = 2) times N, total 1024 elements.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">half</span><span class="o">&gt;</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">gK</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">=</span> <span class="p">{...};</span>

   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a35152c75fb7d3e44aa7e51e9ac25c3b0"><span class="std std-ref">set_scales</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="n">wei_mask</span><span class="p">,</span> <span class="n">wei_groups</span><span class="p">,</span> <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dceaa2449b6477c1fef79be4202906486876"><span class="std std-ref">dnnl::memory::data_type::f16</span></a><span></span><span class="p">);</span>

   <span class="c1">// Additionally, to instruct the library to perform weights dequantization,</span>
   <span class="c1">// fpmath mode must be set with a flag set to `true`:</span>
   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1ab00639157a283596834ee5b0e8478a2d"><span class="std std-ref">set_fpmath_mode</span></a><span></span><span class="p">(</span><a class="reference internal" href="enum_dnnl_fpmath_mode.html#doxid-group-dnnl-api-fpmath-mode-1gga0ad94cbef13dce222933422bfdcfa725aa2449b6477c1fef79be4202906486876"><span class="std std-ref">dnnl::fpmath_mode::f16</span></a><span></span><span class="p">,</span> <span class="cm">/* apply_to_int = */</span> <span class="nb">true</span><span class="p">);</span>

   <span class="c1">// create a matmul primitive descriptor</span>
   <span class="k">auto</span> <span class="n">matmul_pd</span> <span class="o">=</span> <a class="reference internal" href="struct_dnnl_matmul_primitive_desc.html#doxid-structdnnl-1-1matmul-1-1primitive-desc"><span class="std std-ref">dnnl::matmul::primitive_desc</span></a><span></span><span class="p">(</span>
           <span class="n">engine</span><span class="p">,</span>
           <span class="n">src_f16_any_md</span><span class="p">,</span>
           <span class="n">wei_s8_any_md</span><span class="p">,</span>
           <span class="n">dst_f16_any_md</span><span class="p">,</span>
           <span class="n">attr</span><span class="p">);</span>   <span class="c1">// the attributes describe the quantization flow</span>
<span class="c1">// ...</span></pre>
</section>
<section id="matrix-multiplication-with-precomputed-reductions-and-advanced-quantization">
<h3>Matrix Multiplication with Precomputed Reductions and Advanced Quantization<a class="headerlink" href="#matrix-multiplication-with-precomputed-reductions-and-advanced-quantization" title="Permalink to this heading">#</a></h3>
<p>This example extends the <a class="reference external" href="#matrix-multiplication-with-weight-only-quantization-woq">Weight-only Quantization</a> workflow by adding asymmetric weight quantization and external precomputed reductions.</p>
<p>This scenario occurs when quantizing the source tensor at runtime on the application-side, while passing both quantized source and weights to the library.</p>
<p>Precomputed reductions are important when using <code class="docutils literal notranslate"><span class="pre">s8</span></code> zero-points for weights, as applying them during computations would cause accuracy loss.</p>
<pre class="highlight literal-block"><span></span>   <span class="c1">// Src, weights, and dst memory descriptors for matmul.</span>
   <span class="c1">// Consider simple 2D matmul case.</span>
   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">src_u8_any_md</span><span class="p">(</span>
           <span class="p">{</span><span class="n">M</span> <span class="p">(</span><span class="mi">64</span><span class="p">),</span> <span class="n">K</span> <span class="p">(</span><span class="mi">256</span><span class="p">)},</span>            <span class="c1">// dims</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea077393852be20e37026d6281827662f2"><span class="std std-ref">dnnl::memory::data_type::u8</span></a><span></span><span class="p">,</span>  <span class="c1">// the data originally in u8</span>
           <a class="reference internal" href="enum_dnnl_memory_format_tag.html#doxid-structdnnl-1-1memory-1a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec"><span class="std std-ref">dnnl::memory::format_tag::any</span></a><span></span> <span class="c1">// let matmul to choose</span>
           <span class="p">);</span>
   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">wei_s8_any_md</span><span class="p">(</span>
           <span class="p">{</span><span class="n">K</span> <span class="p">(</span><span class="mi">256</span><span class="p">),</span> <span class="n">N</span> <span class="p">(</span><span class="mi">512</span><span class="p">)},</span>           <span class="c1">// dims</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea3e8d88fdd85d7153525e0647cdd97686"><span class="std std-ref">dnnl::memory::data_type::s8</span></a><span></span><span class="p">,</span>  <span class="c1">// the data originally in s8</span>
           <a class="reference internal" href="enum_dnnl_memory_format_tag.html#doxid-structdnnl-1-1memory-1a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec"><span class="std std-ref">dnnl::memory::format_tag::any</span></a><span></span> <span class="c1">// let matmul to choose</span>
           <span class="p">);</span>
   <a class="reference internal" href="struct_dnnl_memory_desc-2.html#doxid-structdnnl-1-1memory-1-1desc"><span class="std std-ref">dnnl::memory::desc</span></a><span></span> <span class="n">dst_f16_any_md</span><span class="p">(...);</span>

   <span class="c1">// prepare the attributes</span>
   <a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr"><span class="std std-ref">dnnl::primitive_attr</span></a><span></span> <span class="n">attr</span><span class="p">;</span>
   <span class="c1">// scale per K and N dimensions:</span>
   <span class="k">const</span> <span class="kt">int</span> <span class="n">wei_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">);</span>
   <span class="c1">// K dimension specifies the group size of `128`. It means that each 128</span>
   <span class="c1">// elements over K dimension will share a single value. For a given example,</span>
   <span class="c1">// there will be two groups, thus, two values referring to a single N value.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">dim_t</span><span class="o">&gt;</span> <span class="n">wei_scales_groups</span> <span class="o">=</span> <span class="p">{</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">}</span>

   <span class="c1">// The scaling factors for quantized weights (as declared above)</span>
   <span class="c1">// A unique scale for each scale_gK (256 / 128 = 2) times N, total 1024</span>
   <span class="c1">// elements.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">half</span><span class="o">&gt;</span> <span class="n">wei_scales</span><span class="p">(</span><span class="n">scale_gK</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">=</span> <span class="p">{...};</span>

   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a35152c75fb7d3e44aa7e51e9ac25c3b0"><span class="std std-ref">set_scales</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="n">wei_mask</span><span class="p">,</span> <span class="n">wei_scales_groups</span><span class="p">,</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dceaa2449b6477c1fef79be4202906486876"><span class="std std-ref">dnnl::memory::data_type::f16</span></a><span></span><span class="p">);</span>

   <span class="c1">// Zero-points would have the same mask as grouping applies for them as well.</span>
   <span class="c1">// For example, let it use the different size of the group.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">dim_t</span><span class="o">&gt;</span> <span class="n">wei_zp_groups</span> <span class="o">=</span> <span class="p">{</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">};</span>

   <span class="c1">// The zero-point factors for quantized weights (as declared above)</span>
   <span class="c1">// A unique zero-point for each zp_gK (256 / 64 = 4) times N, total 2048</span>
   <span class="c1">// elements.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">half</span><span class="o">&gt;</span> <span class="n">wei_zps</span><span class="p">(</span><span class="n">zp_gK</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">=</span> <span class="p">{...};</span>

   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a2a8693f2aba0541ccd59470b41321175"><span class="std std-ref">set_zero_points</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gaf279f28c59a807e71a70c719db56c5b3"><span class="std std-ref">DNNL_ARG_WEIGHTS</span></a><span></span><span class="p">,</span> <span class="n">wei_mask</span><span class="p">,</span> <span class="n">wei_zp_groups</span><span class="p">,</span>
           <a class="reference internal" href="enum_dnnl_memory_data_type.html#doxid-structdnnl-1-1memory-1a8e83474ec3a50e08e37af76c8c075dcea3e8d88fdd85d7153525e0647cdd97686"><span class="std std-ref">dnnl::memory::data_type::s8</span></a><span></span><span class="p">);</span>

   <span class="c1">// Now, specify the precomputed reductions.</span>
   <span class="c1">// Note that it&#39;s specified for source tensor.</span>
   <span class="c1">// It means it should have full-size source tensor mask (which in this</span>
   <span class="c1">// example coincides with `wei_mask`), and groups would be over another</span>
   <span class="c1">// dimension, same as zero-points group size.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">dim_t</span><span class="o">&gt;</span> <span class="n">src_pr_groups</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">};</span>

   <span class="c1">// The precomputed reduction factors for quantized sources.</span>
   <span class="c1">// A unique reduction for each M times pr_gK (256 / 64 = 4), total 256</span>
   <span class="c1">// elements.</span>
   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">half</span><span class="o">&gt;</span> <span class="n">src_prs</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">pr_gK</span><span class="p">)</span> <span class="o">=</span> <span class="p">{...};</span>

   <span class="n">attr</span><span class="p">.</span><a class="reference internal" href="struct_dnnl_primitive_attr-2.html#doxid-structdnnl-1-1primitive-attr-1a24a349d345ac97756a54b01b634b1b3c"><span class="std std-ref">set_precomputed_reductions</span></a><span></span><span class="p">(</span><a class="reference internal" href="group_dnnl_api_primitives_common.html#doxid-group-dnnl-api-primitives-common-1gac37ad67b48edeb9e742af0e50b70fe09"><span class="std std-ref">DNNL_ARG_SRC</span></a><span></span><span class="p">,</span> <span class="n">src_tensor_mask</span><span class="p">,</span>
           <span class="n">src_pr_groups</span><span class="p">);</span>

   <span class="c1">// fpmath mode is not required in case of dynamic quantization as it&#39;s</span>
   <span class="c1">// treated as classical quantization case.</span>

   <span class="c1">// create a matmul primitive descriptor</span>
   <span class="k">auto</span> <span class="n">matmul_pd</span> <span class="o">=</span> <a class="reference internal" href="struct_dnnl_matmul_primitive_desc.html#doxid-structdnnl-1-1matmul-1-1primitive-desc"><span class="std std-ref">dnnl::matmul::primitive_desc</span></a><span></span><span class="p">(</span>
           <span class="n">engine</span><span class="p">,</span>
           <span class="n">src_s8_any_md</span><span class="p">,</span>
           <span class="n">wei_s8_any_md</span><span class="p">,</span>
           <span class="n">dst_f16_any_md</span><span class="p">,</span>
           <span class="n">attr</span><span class="p">);</span>   <span class="c1">// the attributes describe the quantization flow</span>
<span class="c1">// ...</span></pre>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="dev_guide_attributes_dropout.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Dropout</p>
      </div>
    </a>
    <a class="right-next"
       href="dev_guide_attributes_post_ops.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Post-ops</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-model">Quantization Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#static-quantization">Static Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-quantization">Dynamic Quantization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-numerical-behavior-notes">General Numerical Behavior Notes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relevant-apis-and-supported-granularity-levels">Relevant APIs and Supported Granularity Levels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#argument-scaling">Argument Scaling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#available-scaling-api-methods">Available Scaling API Methods</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-scaling-granularity-levels">Supported Scaling Granularity Levels</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#per-tensor-scaling">Per-tensor Scaling</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#per-channel-scaling">Per-Channel Scaling</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#block-scaling">Block Scaling</a><ul class="nav section-nav flex-column">
<li class="toc-h6 nav-item toc-entry"><a class="reference internal nav-link" href="#special-case-mx-compatible-block-scaling-or-dynamic-quantization">Special Case: MX-compatible Block Scaling (or Dynamic Quantization)</a></li>
</ul>
</li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-dimensional-scaling">Multi-Dimensional Scaling</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#argument-zero-points">Argument Zero-Points</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#available-zero-point-api-methods">Available Zero-Point API Methods</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-zero-point-granularity-levels">Supported Zero-Point Granularity Levels</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-case-host-side-scalar-scaling-factor-and-zero-point">Special Case: Host-side Scalar Scaling Factor and Zero-point</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precomputed-reductions">Precomputed Reductions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#available-precomputed-reductions-api-method">Available Precomputed Reductions API Method</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-workflows-examples">Quantization Workflows Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#breakdown-of-convolution-with-int8-quantization">Breakdown of Convolution with INT8 Quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#per-channel-scaling-specifics">Per-Channel Scaling Specifics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weights-preparation-with-per-output-channel-scaling">Weights Preparation with Per-output-channel Scaling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution-with-per-output-channel-quantization">Convolution with Per-output-channel Quantization</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication-with-weight-only-quantization-woq">Matrix Multiplication with Weight-only Quantization (WoQ)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication-with-precomputed-reductions-and-advanced-quantization">Matrix Multiplication with Precomputed Reductions and Advanced Quantization</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
       Copyright 2016-2025 Intel Corporation.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  oneDNN is licensed under Apache License Version 2.0. Refer to the <a href='https://github.com/uxlfoundation/oneDNN/blob/main/LICENSE'>LICENSE</a> file for the full license text and copyright notice.<br><a href='page_legal_information.html'>Legal Information</a>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>