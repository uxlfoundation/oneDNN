# Graph API: Support safe softmax for SDPA

## Background

Attention mask is used in scaled dot-product attention to control which elements
in the attention calculation should be considered or ignored. The mask is
typically applied before the softmax operation to influence the attention
weights. In modern NLP and LLM generation tasks, both padding masks and causal
masks use `-infinity` to mask out (via add or select operation) the token
positions where are not needed for attention.

In cases where the entire sequence is masked out with `-infinity`s, due to the
math of softmax, it will generate `NaN`s for the sequence.

```math
x_i = -inf
```
```math
exp(x_i) = 0
```
```math
sum(exp(x_0), ..., exp(x_s)) = 0
```
```math
exp(x_i) / sum(exp(x_0), ..., exp(x_s)) = NaN
```

Even the user can filter out these `NaN` in the forward path, they cause further
problem in the backward. Related discussions can be found in: [[#1]][1],
[[#2]][2], [[#3]][3].

To solve the problem, PyTorch recently added a "safe" softmax implementation and
called it in its scaled dot-product attention to prevent the `NaN`s after
softmax. It generates zero outputs for the inputs with an entire row of
`-infinity`s. oneDNN is requested to align with this numeric behavior.

## Graph API Proposals

### Proposal 1: Use a large negative value instead of `-infinity` for masks

The `NaN` output is directly caused by the `-infinity` values in attention
masks. One obvious solution is to use a large negative value (eg. -1e9) instead
of `-infinity` for masks. This option has been adopted by some models:
[[#4]][4], [[#5]][5], [[#6]][6], etc. It's also discussed in [[#3]][3]. With
this option, we will not need to change anything in oneDNN.

But the problem of option is also obvious:
- There are more models use `-infinity` as masks. It's not possible to change
  all of them to use other values. There is also an accuracy concern for it.
- As PyTorch also changed to the new implementation, there is an inconsistency
  between oneDNN and PyTorch which makes oneDNN integration fails in PyTorch
  tests.

### Proposal 2: Add an attribute `safe` to oneDNN softmax operation

As the `NaN` is generated by softmax and softmax is used in oneDNN API to
construct SDPA patterns, an option is to add an attribute to oneDNN softmax
operation to specify the computation behavior. When constructing an SDPA
pattern, the user can set the attribute indicating oneDNN to do softmax with
returning zeros for entire masked rows or unset the attribute to have a normal
softmax. This is similar to PyTorch which provides both softmax and safe_softmax
implementations while in oneDNN, we use an attribute to differentiate the
behaviors.

To simplify the API change, we can reuse the existing operation attribute
`attr::mode` for softmax. The attribute definition for softmax is as below:

| Attribute Name   | Description                            | Value Type | Supported Values                                           | Required or Optional |
|:-----------------|:---------------------------------------|:-----------|:-----------------------------------------------------------|:---------------------|
| mode             | Specify the algorithm used by softmax. | string     | "none": normal softmax (default)                           | Optional             |
|                  |                                        |            | "inf_as_zero": return zeros if all `-infinity`s in the row |                      |

```c++
    auto softmax = op(id++, op::kind::SoftMax, "softmax");
    softmax.set_attr<int64_t>(op::attr::axis, -1);
    softmax.set_attr<std::string>(op::attr::mode, "inf_as_zero");
    softmax.add_inputs({masked_score});
    softmax.add_outputs({probs});
```

For performance considerations, backend implementations will not need to scan
the input of softmax to decide if there are entirely masked out rows, instead
they can perform the check right before the division to minimize the performance
impact:

```math
acc = sum(exp(x_0), ..., exp(x_s))
```
```math
o_i = acc == 0 \; ? \; 0 \; : \; (exp(x_i) / acc)
```

With this option, we will have the same numeric behavior between oneDNN and
PyTorch.

One potential issue with this option is that the `-infinity`s may come from the
attention score, rather than the attention mask. In this case, we will still
return zeros for the rows with all `-infinity`s. It's possible that the user
wants to propagate these special values to the output and the following layers.
But it seems this is not considered in PyTorch's decision in [[#3]][3].

### Option 3: Add an attribute to oneDNN Add/Select operations

To address the potential issue in option 2, we will need to know where the
`-infinity`s come from. This option proposes to add an attribute to the Add and
Select operation which are used to apply the attention mask to the attention
score as they know if the mask contains `-infinity`s. With the attribute being
set, if the `-infinity` comes from the attention score side, the masked output
will be `-infinity`, otherwise if the `-infinity` comes from the the attention
mask side, a large negative value (eg. -1e9) will be applied the score instead.
The following softmax layer will be kept unchanged. In this case, we will be
able to propagate the special value to softmax's output if it comes from
attention score.

But with this option, the problem is for entire row with `-infinity` mask, if
the large negative value is not large enough, softmax will generate the same
probability (1.f / dim) for all elements. It's different from the zeros
generated by safe softmax. If the large negative value is large enough, we will
still see `NaN`s after a normal softmax, unless we change the implementation of
softmax to detect these special values.

```python

>>> a = torch.tensor([-1e9, -1e9, -1e9, -1e9])
>>> torch.nn.Softmax(dim=0)(a)
tensor([0.2500, 0.2500, 0.2500, 0.2500])

```

### Conclusion

Option 2 is more preferred based on the above discussion.

## Primitive Proposal

### Option 1: Introduce a new algorithm kind

For a Graph SDPA implementation consisting of primitives to dispatch into a
proper softmax implementation an additional marker on a softmax primitive side
is required. While for the library side it is enough to have this marker not
exposed in public API, for benchdnn validation it's nice to have it publicly
available to be able to dispatch into "inf_as_zero" implementation on
the f32 path consisting of primitives. Otherwise, it will be impossible to
achieve identical results. However, there's a way to use the internal value, in
case when internal header containing the value is not available, and the
proposal is to rely on it until there's a strong public request to have this
algorithm exposed for a standalone softmax primitive, or until the number of
issues associated with the fact of internal value usage outnumbers the benefit
of keeping the algorithm internally.

```c++
/// Kinds of algorithms
typedef enum {
    ...
    /// Softmax
    dnnl_softmax_accurate = 0x30000,
    /// Logsoftmax
    dnnl_softmax_log,
    /// Softmax treating all infinity inputs as zero
    dnnl_softmax_inf_as_zero,
} dnnl_alg_kind_t;
```

## References

1. https://github.com/pytorch/pytorch/issues/103749
2. https://github.com/pytorch/pytorch/issues/103963
3. https://github.com/pytorch/pytorch/pull/133882
4. https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L712
5. https://github.com/huggingface/transformers/blob/4542b8fb271be54d9235ac7a899c6aab2101612f/tests/utils/test_modeling_tf_utils.py#L171
6. https://github.com/huggingface/transformers/blob/dd3933dd658b2c2e18ad316662a3dff09dcf98cb/src/transformers/models/superglue/modeling_superglue.py#L737

[1]: https://github.com/pytorch/pytorch/issues/103749
[2]: https://github.com/pytorch/pytorch/issues/103963
[3]: https://github.com/pytorch/pytorch/pull/133882
[4]: https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L712
[5]: https://github.com/huggingface/transformers/blob/4542b8fb271be54d9235ac7a899c6aab2101612f/tests/utils/test_modeling_tf_utils.py#L171
[6]: https://github.com/huggingface/transformers/blob/dd3933dd658b2c2e18ad316662a3dff09dcf98cb/src/transformers/models/superglue/modeling_superglue.py#L737
